\documentclass[12pt letter]{report}
\input{./template/preamble}
\input{./template/macros}
\input{./template/letterfonts}

\setcounter{tocdepth}{4}
\setcounter{secnumdepth}{4}
\usepackage{parskip}


    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Keep aspect ratio if custom image width or height is specified
    \setkeys{Gin}{keepaspectratio}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro

    \usepackage{iftex}
    \ifPDFTeX
        \usepackage[T1]{fontenc}
        \IfFileExists{alphabeta.sty}{
              \usepackage{alphabeta}
          }{
              \usepackage[mathletters]{ucs}
              \usepackage[utf8x]{inputenc}
          }
    \else
        \usepackage{fontspec}
    \fi

    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{array}     % table support for pandoc >= 2.11.3
    \usepackage{calc}      % table minipage width calculation for pandoc >= 2.11.1
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{\Huge{Neural Networks}}
    
\date{}

    
\author{\huge{Madiba Hudson-Quansah}}

    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@ges}{\let\PY@bf=\textbf\let\PY@it=\textit}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb.
    \makeatletter
        \newbox\Wrappedcontinuationbox
        \newbox\Wrappedvisiblespacebox
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}}
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}}
        \newcommand*\Wrappedcontinuationindent {3ex }
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox}
        % Take advantage of the already applied Pygments mark-up to insert
        % potential linebreaks for TeX processing.
        %        {, <, #, %, $, ' and ": go to next line.
        %        _, }, ^, &, >, - and ~: stay at end of broken line.
        % Use of \textquotesingle for straight quote.
        \newcommand*\Wrappedbreaksatspecials {%
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}%
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}%
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}%
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}%
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}%
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}%
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}%
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}%
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}%
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}%
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}%
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}%
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}%
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}%
        }
        % Some characters . , ; ? ! / are not pygmentized.
        % This macro makes them "active" and they will insert potential linebreaks
        \newcommand*\Wrappedbreaksatpunct {%
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}%
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}%
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}%
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}%
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}%
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}%
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}%
            \catcode`\.\active
            \catcode`\,\active
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active
            \lccode`\~`\~
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%

        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}

    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
\newpage
\pdfbookmark[section]{\contentsname}{too}
    
    \maketitle
    
    
    \tableofcontents
\newpage


    
\chapter{Artificial Neural Networks}\label{artificial-neural-networks}

An Artificial Neural Network (ANN) is a computational model that is
inspired by the way biological neural networks in the human brain work.
It is composed of a large number of highly interconnected processing
elements (neurons) working in unison to solve specific problems. ANNs,
like people, learn by example and are thus capable of solving
unsupervised as well as supervised learning tasks.

\section{Biological Neurons}\label{biological-neurons}

Artificial Neural Networks being inspired by biological neural networks
are composed of neurons, thus to understand the decision designs of ANNs
we must first understand the biological neuron. Neurons are made up of
three parts, the \emph{soma} or the cell body, the \emph{axon} and
\emph{dendrites} A neuron is a specialized cell responsible for the
creation and transportation of short electrical impulses called
\emph{action potentials} / AP / signals, used to communicate with other
neurons with the overarching goal of transmitting information throughout
the body. Through this vast network of neurons, the brain is able to
perform highly complex computations. Research shows that neurons in the
brain are organized in consecutive layers with each layer performing a
specific task. This is the basis of the design of ANNs.

\section{Logical Computation with
Neurons}\label{logical-computation-with-neurons}

The Artificial Neuron is designed on this framework of the biological
neuron, with a simple ANN proposed by McCulloch and Pitts, having one or
more binary inputs and one binary output. The artificial neuron
activates its output when more than a certain number of its inputs are
active, allowing for simple computation of logical functions, such as
propositional logic. For example a simple neuron designed to perform the
logical AND operation:

Let the output neuron be \(C\) and take two input neuron \(A\) and
\(B\). As the neuron \(C\) always requires two input signals to give an
output signal we can then require the two input signals to only give one
output signal if activated. This would in turn require both neurons
\(A\) and \(B\) to be activated to give an output signal for neuron
\(C\), thus performing the logical AND operation.

\section{The Perceptron}\label{the-perceptron}

A simple ANN, based on a artificial neuron called a \emph{threshold
logic unit} (TLU) / \emph{linear threshold unit} (LTU). It inputs and
outputs numbers and each input connection is associated with a weight.
The TLU first computes a linear function of its inputs \[
z = w_1 x_1 + \ldots + w_n x_n + b = \mathbf{w}^T \mathbf{x} + b
\] Where the model parameters are the input weights \(\mathbf{w}\) and
\(b\) is the bias term

It then applies a \textbf{step function} to the result \[
h_w(\mathbf{x}) = \text{step}(z)
\] A common step function used in perceptrons is the \emph{Heavyside
step function}, and the \emph{sign function} defined: \[
\text{heavyside}(z) = \begin{cases}
0 & \text{ if } z < 0 \\
1 & \text{ if } z \ge 0
\end{cases}
\]

\[
\text{sgn}(z) = \begin{cases}
-1 & \text{ if } z < 0 \\
0 & \text{ if } z = 0 \\
+1 & \text{ if } z > 0
\end{cases}
\]

A perceptron is composed of one or more TLUs organized in a single
layer, where every TLU is connected to every input. Such a layer is
called a fully \textbf{connected layer} or a \textbf{dense layer}, and
the inputs constitute the \textbf{input layer}. As the final layer of
TLU's produce the output this layer is called the \textbf{output layer}.

Using linear algebra the outputs of nodes in the output layers, for
several instances at once, i.e.: \[
h_{W, b} (X) = \phi (XW + b)
\]

Where:

\begin{itemize}
\item
  \(W\) is a matrix containing all the connection weights, with one row
  per unit and one column per node.
\item
  \(X\) is a matrix containing the input features, with noe row per
  instance and one colum per feature, where and instance is a data
  point.
\item
  \(b\) is the bias vector that contains all the bias terms, one entry
  per neuron, where whe entires in \(b\) are broadcasted across the rows
  of \(XW\).
\item
  \(\phi\) is the \textbf{activation function}, but when the artificial
  neurons are TLUs this is termed as the step function
\end{itemize}

The perceptron training algorithm is largely inspired by Hebb's rule,
which suggests what when neurons on either side of a synapse or in the
case of artificial neurons connection, fire together the connection
between them is strengthened. This results in the strengthening of
connections that help reduce the error in the output layer.

The perceptron is fed one training instance at a time and for each
instance it makes predictions. For every output neuron that produced a
wrong prediction, it reinforces the connection weights from the inputs
that would have contributed to the correct prediction. The rule is as
follows:

\[
w_{i, j}^{\text{next step}} = w_{i,j} + \eta (y_j - \hat{y_j})x_i
\]

Where:

\begin{itemize}
\item
  \(w_{i,j}\) is the connection weight between the \(i\) th input neuron
  and the \(j\) th neuron.
\item
  \(x_i\) is the \(i\) th input value of the current training instance.
\item
  \(\hat{y_j}\) is the output of the \(j\) th output neuron for the
  current training instance.
\item
  \(y_j\) is the target output of the \(j\) th output neuron for the
  current training instance.
\item
  \(\eta\) is the learning rate.
\end{itemize}

The decision boundary of each output neuron is linear, so perceptrons
are incapable of learning complex patterns (just like logistic
regression classifiers). However, if the training instances are linearly
separable, Rosenblatt demonstrated that this algorithm would converge to
a solution.⁠ This is called the \textbf{perceptron convergence theorem}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{sys}

\PY{o}{!}\PY{o}{\PYZob{}}sys.executable\PY{o}{\PYZcb{}}\PY{+w}{ }\PYZhy{}m\PY{+w}{ }pip\PY{+w}{ }install\PY{+w}{ }pydot
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Requirement already satisfied: pydot in
/home/mads/.pyenv/versions/ai/lib/python3.12/site-packages (2.0.0)
Requirement already satisfied: pyparsing>=3 in
/home/mads/.pyenv/versions/ai/lib/python3.12/site-packages (from pydot) (3.1.2)
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{datasets} \PY{k+kn}{import} \PY{n}{load\PYZus{}iris}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k+kn}{import} \PY{n}{Perceptron}

\PY{n}{iris} \PY{o}{=} \PY{n}{load\PYZus{}iris}\PY{p}{(}\PY{n}{as\PYZus{}frame}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{X} \PY{o}{=} \PY{n}{iris}\PY{o}{.}\PY{n}{data}\PY{p}{[}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{petal length (cm)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{petal width (cm)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{values}
\PY{n}{y} \PY{o}{=} \PY{n}{iris}\PY{o}{.}\PY{n}{target} \PY{o}{==} \PY{l+m+mi}{0}

\PY{n}{per} \PY{o}{=} \PY{n}{Perceptron}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{)}
\PY{n}{per}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{)}

\PY{n}{XNew} \PY{o}{=} \PY{p}{[}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}
\PY{n}{per}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{XNew}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
array([ True, False])
\end{Verbatim}
\end{tcolorbox}
        
\section{Multilayer Perceptron (MLP) and
Backpropagation}\label{multilayer-perceptron-mlp-and-backpropagation}

An MLP is composed of one input layer, one or more layers of TLUs called
\emph{hidden layers} and one output layer, also comprised of TLUs.
Layers close the input layers are called the \emph{lower layers} and
those close to the output layers are called \emph{upper layers}.

When signals only flow from the input layer across, hidden layers and
finally stopping at the output layers, i.e one direction, the resulting
architecture is called a \textbf{Feedforward Neural Network (FNN)}.

When an ANN contains a deep stack of hidden layers, it is called a
\textbf{Deep Neural Network (DNN)}. A deep stack is when the number of
hidden layers is large, typically more than 10.

Using the \emph{reverse-mode automatic differentiation / reverse-mode
autodiff} algorithm the gradient of a ANN's error in relation to the
parameters it's given can be computed. Using this gradient it is
possible to perform gradient descent to find the optimal weights and
biases for the ANN, i.e.~the parameters that minimize the error. This
combination of both the reverse-mode autodiff and gradient descent is
called \textbf{Backpropagation}.

Going into detail the Backpropagation algorithm is as follows:

\begin{itemize}
\item
  A mini-batch (for example 32 instances) of the training data is used
  at a time, and the algorithm goes through the full training data
  several times. Each time through the full training data is called an
  \textbf{epoch}.
\item
  Each mini-batch enters the network through the input layer, then the
  output of the all the neurons in the first hidden layer is computed
  for each instance in the mini-batch. The result is then passed to the
  subsequent hidden layer and computed so on until the output layer is
  reached. This is called a \textbf{forward pass}, where the model makes
  predictions but all the intermediate results between hidden layers are
  stored for use in the backwards pass.
\item
  The algorithm then calculates the model's output error, using a loss
  function that compares the result to the desired output of the network
  for the given parameters and returns the measure of the error.
\item
  Then it computes how much each output bias and each connection to the
  output layer contributed to the error, by analytically applying the
  chain rule.
\item
  The algorithm then measures how much of these error contributions came
  from each connection in the layer below, again using the chain rule,
  working backwards until it reaches the input layer. This is called the
  \textbf{backward / reverse pass}.
\item
  Finally the algorithm performs a gradient descent step to tweak all
  the connections in the network, using the error contributions computed
  earlier. This is called the \textbf{gradient descent step}.
\end{itemize}

In order for Backpropagation to work properly it needs a gradient to
work with, thus the step functions normally used in single layer
perceptrons are replaced with differentiable functions, such as the
\textbf{logistic / sigmoid function}, \textbf{hyperbolic tangent
function} and the \textbf{rectified linear unit function (ReLU)},
defined below:

\[
\text{sigmoid}(z) = \frac{1}{1 + \exp(-z)}
\]

\[
\tanh(z) = 2 \sigma(2z) - 1
\]

\[
\text{ReLU}(z) = \max(0, z)
\]

\section{Regression MLPs}\label{regression-mlps}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.4667}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Hyperparameter
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Typical Value
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\# hidden layers & Depends on the problem typically 1 - 5 \\
\# neurons per hidden layer & Depends on the problem typically 10 -
100 \\
\# output neurons & 1 per prediction dimension \\
hidden activation & ReLu \\
output activation & None, or ReLU/ softplus if positive outputs or
sigmoid / tanhn if bounded \\
Loss function & MSE, or huber if outliers \\
\end{longtable}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{datasets} \PY{k+kn}{import} \PY{n}{fetch\PYZus{}california\PYZus{}housing}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{neural\PYZus{}network} \PY{k+kn}{import} \PY{n}{MLPRegressor}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{pipeline} \PY{k+kn}{import} \PY{n}{Pipeline}\PY{p}{,} \PY{n}{make\PYZus{}pipeline}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k+kn}{import} \PY{n}{StandardScaler}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k+kn}{import} \PY{n}{root\PYZus{}mean\PYZus{}squared\PYZus{}error}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k+kn}{import} \PY{n}{train\PYZus{}test\PYZus{}split}

\PY{n}{housing} \PY{o}{=} \PY{n}{fetch\PYZus{}california\PYZus{}housing}\PY{p}{(}\PY{p}{)}
\PY{n}{XTrainFull}\PY{p}{,} \PY{n}{XTest}\PY{p}{,} \PY{n}{yTrainFull}\PY{p}{,} \PY{n}{yTest} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}
    \PY{n}{housing}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{n}{housing}\PY{o}{.}\PY{n}{target}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}
\PY{p}{)}
\PY{n}{XTrain}\PY{p}{,} \PY{n}{XValid}\PY{p}{,} \PY{n}{yTrain}\PY{p}{,} \PY{n}{yValid} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}
    \PY{n}{XTrainFull}\PY{p}{,} \PY{n}{yTrainFull}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}
\PY{p}{)}

\PY{n}{mlpReg} \PY{o}{=} \PY{n}{MLPRegressor}\PY{p}{(}
    \PY{n}{hidden\PYZus{}layer\PYZus{}sizes}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{50}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{]}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{relu}\PY{l+s+s2}{\PYZdq{}}
\PY{p}{)}
\PY{n}{mlpRegPipe} \PY{o}{=} \PY{n}{make\PYZus{}pipeline}\PY{p}{(}\PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{mlpReg}\PY{p}{)}
\PY{n}{mlpRegPipe}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{XTrain}\PY{p}{,} \PY{n}{yTrain}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
Pipeline(steps=[('standardscaler', StandardScaler()),
                ('mlpregressor',
                 MLPRegressor(hidden\_layer\_sizes=[50, 50, 50],
                              random\_state=42))])
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{preds} \PY{o}{=} \PY{n}{mlpRegPipe}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{XValid}\PY{p}{)}
\PY{n}{rmse} \PY{o}{=} \PY{n}{root\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{yValid}\PY{p}{,} \PY{n}{preds}\PY{p}{)}
\PY{n}{rmse}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
0.5053326657968522
\end{Verbatim}
\end{tcolorbox}
        
\section{Classification MLPs}\label{classification-mlps}

For binary classification the output neuron has to have a bounded
activation function like sigmoid so the output will be a number between
0 and 1, which can be interpreted as the estimated probability of the
positive class. The estimated probability of the negative class is equal
to one minus that number.

For multilabel binary classification the output layer should have one
output neuron per class, and each neuron should have a sigmoid
activation function, where the first neuron predicts the probability of
the first class, the second neuron the probability of the second class
and so on.

If each instance can only belong to one category out of all the possible
categories, then the output layer should have one neuron per class and
use the \textbf{softmax} activation function for the whole output layer.
This ensures that the sum of all the estimated class probabilities for
each instance is equal to one, and that the estimated class
probabilities are all non-negative.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1633}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2449}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3367}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2551}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Hyperparameter
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Binary Classification
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Multilabel Binary Classification
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Multiclass Classification
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\# hidden layers & Typically 1 - 5 layers, depending on the task &
Typically 1 - 5 layers, depending on the task & Typically 1 - 5 layers,
depending on the task \\
\# output neurons & 1 & 1 per binary label & 1 per class \\
Output activation & Sigmoid & Sigmoid & Softmax \\
Loss function & Cross-entropy & Cross-entropy & Cross-entropy \\
\end{longtable}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{datasets} \PY{k+kn}{import} \PY{n}{load\PYZus{}iris}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{neural\PYZus{}network} \PY{k+kn}{import} \PY{n}{MLPClassifier}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k+kn}{import} \PY{n}{classification\PYZus{}report}\PY{p}{,} \PY{n}{ConfusionMatrixDisplay}

\PY{n}{iris} \PY{o}{=} \PY{n}{load\PYZus{}iris}\PY{p}{(}\PY{p}{)}
\PY{n}{XTrainFull}\PY{p}{,} \PY{n}{XTest}\PY{p}{,} \PY{n}{yTrainFull}\PY{p}{,} \PY{n}{yTest} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}
    \PY{n}{iris}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{n}{iris}\PY{o}{.}\PY{n}{target}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}
\PY{p}{)}
\PY{n}{XTrain}\PY{p}{,} \PY{n}{XValid}\PY{p}{,} \PY{n}{yTrain}\PY{p}{,} \PY{n}{yValid} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}
    \PY{n}{XTrainFull}\PY{p}{,} \PY{n}{yTrainFull}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}
\PY{p}{)}

\PY{n}{mlpCls} \PY{o}{=} \PY{n}{MLPClassifier}\PY{p}{(}\PY{n}{hidden\PYZus{}layer\PYZus{}sizes}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{10}\PY{p}{]}\PY{p}{,} \PY{n}{max\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{10000}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{)}
\PY{n}{mlpClsPipe} \PY{o}{=} \PY{n}{make\PYZus{}pipeline}\PY{p}{(}\PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{mlpCls}\PY{p}{)}
\PY{n}{mlpClsPipe}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{XTrain}\PY{p}{,} \PY{n}{yTrain}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
Pipeline(steps=[('standardscaler', StandardScaler()),
                ('mlpclassifier',
                 MLPClassifier(hidden\_layer\_sizes=[10], max\_iter=10000,
                               random\_state=42))])
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{preds} \PY{o}{=} \PY{n}{mlpClsPipe}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{XTest}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{yTest}\PY{p}{,} \PY{n}{preds}\PY{p}{)}\PY{p}{)}
\PY{n}{ConfusionMatrixDisplay}\PY{o}{.}\PY{n}{from\PYZus{}predictions}\PY{p}{(}\PY{n}{yTest}\PY{p}{,} \PY{n}{preds}\PY{p}{,} \PY{n}{display\PYZus{}labels}\PY{o}{=}\PY{n}{iris}\PY{o}{.}\PY{n}{target\PYZus{}names}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
              precision    recall  f1-score   support

           0       1.00      1.00      1.00        15
           1       1.00      1.00      1.00        11
           2       1.00      1.00      1.00        12

    accuracy                           1.00        38
   macro avg       1.00      1.00      1.00        38
weighted avg       1.00      1.00      1.00        38

    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<sklearn.metrics.\_plot.confusion\_matrix.ConfusionMatrixDisplay at
0x7e520c2cf560>
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Neural Networks_files/Neural Networks_9_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
\section{Implementing MLPs in Keras}\label{implementing-mlps-in-keras}

\subsection{Building an Image Classifier using the Sequential
API}\label{building-an-image-classifier-using-the-sequential-api}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{tensorflow} \PY{k}{as} \PY{n+nn}{tf}

\PY{n}{fmnist} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{datasets}\PY{o}{.}\PY{n}{fashion\PYZus{}mnist}\PY{o}{.}\PY{n}{load\PYZus{}data}\PY{p}{(}\PY{p}{)}
\PY{p}{(}\PY{n}{XTrainFull}\PY{p}{,} \PY{n}{yTrainFull}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{n}{XTest}\PY{p}{,} \PY{n}{yTest}\PY{p}{)} \PY{o}{=} \PY{n}{fmnist}

\PY{n}{cutoff} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{5000}
\PY{n}{XTrain}\PY{p}{,} \PY{n}{yTrain} \PY{o}{=} \PY{n}{XTrainFull}\PY{p}{[}\PY{p}{:}\PY{n}{cutoff}\PY{p}{]}\PY{p}{,} \PY{n}{yTrainFull}\PY{p}{[}\PY{p}{:}\PY{n}{cutoff}\PY{p}{]}
\PY{n}{XValid}\PY{p}{,} \PY{n}{yValid} \PY{o}{=} \PY{n}{XTrainFull}\PY{p}{[}\PY{n}{cutoff}\PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{yTrainFull}\PY{p}{[}\PY{n}{cutoff}\PY{p}{:}\PY{p}{]}
\PY{n}{XTrain}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
2024-07-13 23:17:06.657046: I tensorflow/core/util/port.cc:113] oneDNN custom
operations are on. You may see slightly different numerical results due to
floating-point round-off errors from different computation orders. To turn them
off, set the environment variable `TF\_ENABLE\_ONEDNN\_OPTS=0`.
2024-07-13 23:17:06.664028: I external/local\_tsl/tsl/cuda/cudart\_stub.cc:32]
Could not find cuda drivers on your machine, GPU will not be used.
2024-07-13 23:17:06.745207: I external/local\_tsl/tsl/cuda/cudart\_stub.cc:32]
Could not find cuda drivers on your machine, GPU will not be used.
2024-07-13 23:17:06.881027: E
external/local\_xla/xla/stream\_executor/cuda/cuda\_fft.cc:479] Unable to register
cuFFT factory: Attempting to register factory for plugin cuFFT when one has
already been registered
2024-07-13 23:17:06.986610: E
external/local\_xla/xla/stream\_executor/cuda/cuda\_dnn.cc:10575] Unable to
register cuDNN factory: Attempting to register factory for plugin cuDNN when one
has already been registered
2024-07-13 23:17:06.987620: E
external/local\_xla/xla/stream\_executor/cuda/cuda\_blas.cc:1442] Unable to
register cuBLAS factory: Attempting to register factory for plugin cuBLAS when
one has already been registered
2024-07-13 23:17:07.182248: I tensorflow/core/platform/cpu\_feature\_guard.cc:210]
This TensorFlow binary is optimized to use available CPU instructions in
performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512\_VNNI FMA, in other
operations, rebuild TensorFlow with the appropriate compiler flags.
2024-07-13 23:17:09.164982: W
tensorflow/compiler/tf2tensorrt/utils/py\_utils.cc:38] TF-TRT Warning: Could not
find TensorRT
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
(55000, 28, 28)
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{XTrain}\PY{p}{,} \PY{n}{XValid}\PY{p}{,} \PY{n}{XTest} \PY{o}{=} \PY{n}{XTrain} \PY{o}{/} \PY{l+m+mf}{255.0}\PY{p}{,} \PY{n}{XValid} \PY{o}{/} \PY{l+m+mf}{255.0}\PY{p}{,} \PY{n}{XTest} \PY{o}{/} \PY{l+m+mf}{255.0}
\PY{n}{classNames} \PY{o}{=} \PY{p}{[}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{T\PYZhy{}shirt/top}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Trouser}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Pullover}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Dress}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Coat}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Sandal}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Shirt}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Sneaker}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Bag}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Ankle boot}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
\PY{p}{]}


\PY{k}{def} \PY{n+nf}{name}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n}{classNames}\PY{p}{[}\PY{n}{x}\PY{p}{]}


\PY{n}{name}\PY{p}{(}\PY{n}{yTrain}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
'Ankle boot'
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Set random seed for reproducible results}
\PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{utils}\PY{o}{.}\PY{n}{set\PYZus{}random\PYZus{}seed}\PY{p}{(}\PY{l+m+mi}{42}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Create a Sequential model a single stack of layers connected sequentially}
\PY{n+nb+bp}{cls} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} The first layer, an Input layer, added to the model with shape}
\PY{c+c1}{\PYZsh{} 28x28, needed to determine the shape of the weight matrix of te}
\PY{c+c1}{\PYZsh{} first hidden layer}
\PY{n+nb+bp}{cls}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{28}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{]}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} A Flatten layer to convert the input images into 1D arrays}
\PY{n+nb+bp}{cls}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Flatten}\PY{p}{(}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} A Dense layer with 300 neurons, using the ReLU activation function}
\PY{c+c1}{\PYZsh{} Each Dense layer manages it\PYZsq{}s own weight matrix containing the connection weights}
\PY{c+c1}{\PYZsh{} between neurons and their inputs}
\PY{n+nb+bp}{cls}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{300}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{relu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} A second Dense layer with 100 neurons, also using the ReLU activation function}
\PY{n+nb+bp}{cls}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{relu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} A final Dense layer as the output layer with 10 neurons, one per category}
\PY{c+c1}{\PYZsh{} And using the softmax activation function as classes are exclusive}
\PY{n+nb+bp}{cls}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{softmax}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
\PY{n+nb+bp}{cls}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{Model: "sequential"}

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃\textbf{ }\textbf{Layer (type)                   }\textbf{ }┃\textbf{ }\textbf{Output Shape          }\textbf{ }┃\textbf{ }\textbf{      Param \#}\textbf{ }┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ flatten (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Flatten})               │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{784})            │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                   │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{300})            │       \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{235,500} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_1 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                 │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{100})            │        \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{30,100} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_2 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                 │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{10})             │         \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1,010} │
└─────────────────────────────────┴────────────────────────┴───────────────┘

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Total params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{266,610} (1.02 MB)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{266,610} (1.02 MB)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Non-trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0} (0.00 B)

    \end{Verbatim}

    
These steps can be condensed as seen below with the first \texttt{Input}
layer dropped and the \texttt{input\_shape} specified in the first layer

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb+bp}{cls} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}
    \PY{p}{[}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{28}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{]}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Flatten}\PY{p}{(}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{300}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{leaky\PYZus{}relu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{leaky\PYZus{}relu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{softmax}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{,}
    \PY{p}{]}
\PY{p}{)}
\PY{n+nb+bp}{cls}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{Model: "sequential\_1"}

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃\textbf{ }\textbf{Layer (type)                   }\textbf{ }┃\textbf{ }\textbf{Output Shape          }\textbf{ }┃\textbf{ }\textbf{      Param \#}\textbf{ }┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ flatten\_1 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Flatten})             │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{784})            │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_3 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                 │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{300})            │       \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{235,500} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_4 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                 │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{100})            │        \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{30,100} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_5 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                 │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{10})             │         \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1,010} │
└─────────────────────────────────┴────────────────────────┴───────────────┘

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Total params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{266,610} (1.02 MB)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{266,610} (1.02 MB)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Non-trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0} (0.00 B)

    \end{Verbatim}

    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{display}\PY{p}{(}\PY{n+nb+bp}{cls}\PY{o}{.}\PY{n}{layers}\PY{p}{)}
\PY{n}{hid1} \PY{o}{=} \PY{n+nb+bp}{cls}\PY{o}{.}\PY{n}{layers}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
\PY{n}{w}\PY{p}{,} \PY{n}{b} \PY{o}{=} \PY{n}{hid1}\PY{o}{.}\PY{n}{get\PYZus{}weights}\PY{p}{(}\PY{p}{)}
\PY{n}{b}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{Verbatim}[commandchars=\\\{\}]
[<Flatten name=flatten\_1, built=True>,
 <Dense name=dense\_3, built=True>,
 <Dense name=dense\_4, built=True>,
 <Dense name=dense\_5, built=True>]
    \end{Verbatim}

    
            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)
\end{Verbatim}
\end{tcolorbox}
        
\subsubsection{Compiling the Model}\label{compiling-the-model}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Loss \PYZhy{} Specifies the loss function to use when minimizing the value of the loss function}
\PY{c+c1}{\PYZsh{} Optimize \PYZhy{} Specifies the optimization algorithm to use in reducing the loss function}
\PY{c+c1}{\PYZsh{} Metrics \PYZhy{} Specifies other metrics to monitor during training and testing}
\PY{n+nb+bp}{cls}\PY{o}{.}\PY{n}{compile}\PY{p}{(}
    \PY{n}{loss}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{sparse\PYZus{}categorical\PYZus{}crossentropy}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
    \PY{n}{optimizer}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{optimizers}\PY{o}{.}\PY{n}{SGD}\PY{p}{(}\PY{p}{)}\PY{p}{,}
    \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{accuracy}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,}
\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

\begin{itemize}
\item
  We use \texttt{sparse\_categorical\_crossentropy} loss as we have
  sparse labels, for each instance there is just a target class index
  from 0 to 9, and the classes are exclusive. If we instead had one
  target probabilty per class for each instance for example when using
  One Hot Encoding, we would use \texttt{categorical\_crossentropy}
  loss. When doing binary classification or multilabel binary
  classification we would use the \texttt{binary\_crossentropy} loss
\item
  The optimizer \texttt{sgd} stands for \emph{Stochastic Gradient
  Descent}, and means the model will be trained using this gradient
  descent variant. The \texttt{sgd} optimizer will perform the
  backpropagation algorithm. When using the \texttt{sdg} optimizer it is
  important to set a learning rate, this can be done by passing
  \texttt{tf.keras.optimizers.SGD(learning\_rate=x)} where \texttt{x} is
  the learning rate
\end{itemize}

\subsubsection{Training and Evaluating the
Model}\label{training-and-evaluating-the-model}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{hist} \PY{o}{=} \PY{n+nb+bp}{cls}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{XTrain}\PY{p}{,} \PY{n}{yTrain}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{30}\PY{p}{,} \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{XValid}\PY{p}{,} \PY{n}{yValid}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Epoch 1/30
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{5s} 2ms/step -
accuracy: 0.6862 - loss: 0.9684 - val\_accuracy: 0.8304 - val\_loss: 0.5021
Epoch 2/30
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{3s} 2ms/step -
accuracy: 0.8262 - loss: 0.5073 - val\_accuracy: 0.8400 - val\_loss: 0.4540
Epoch 3/30
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{4s} 2ms/step -
accuracy: 0.8420 - loss: 0.4570 - val\_accuracy: 0.8466 - val\_loss: 0.4333
Epoch 4/30
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{4s} 2ms/step -
accuracy: 0.8505 - loss: 0.4295 - val\_accuracy: 0.8470 - val\_loss: 0.4207
Epoch 5/30
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{3s} 2ms/step -
accuracy: 0.8570 - loss: 0.4097 - val\_accuracy: 0.8506 - val\_loss: 0.4106
Epoch 6/30
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{3s} 2ms/step -
accuracy: 0.8618 - loss: 0.3941 - val\_accuracy: 0.8528 - val\_loss: 0.4020
Epoch 7/30
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{3s} 2ms/step -
accuracy: 0.8670 - loss: 0.3810 - val\_accuracy: 0.8552 - val\_loss: 0.3963
Epoch 8/30
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{3s} 2ms/step -
accuracy: 0.8704 - loss: 0.3696 - val\_accuracy: 0.8564 - val\_loss: 0.3912
Epoch 9/30
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{3s} 2ms/step -
accuracy: 0.8732 - loss: 0.3596 - val\_accuracy: 0.8582 - val\_loss: 0.3856
Epoch 10/30
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{4s} 3ms/step -
accuracy: 0.8758 - loss: 0.3507 - val\_accuracy: 0.8602 - val\_loss: 0.3817
Epoch 11/30
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{3s} 2ms/step -
accuracy: 0.8784 - loss: 0.3426 - val\_accuracy: 0.8634 - val\_loss: 0.3768
Epoch 12/30
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{4s} 2ms/step -
accuracy: 0.8806 - loss: 0.3350 - val\_accuracy: 0.8644 - val\_loss: 0.3739
Epoch 13/30
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{3s} 2ms/step -
accuracy: 0.8829 - loss: 0.3280 - val\_accuracy: 0.8654 - val\_loss: 0.3708
Epoch 14/30
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{3s} 2ms/step -
accuracy: 0.8845 - loss: 0.3213 - val\_accuracy: 0.8658 - val\_loss: 0.3687
Epoch 15/30
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{3s} 2ms/step -
accuracy: 0.8863 - loss: 0.3152 - val\_accuracy: 0.8668 - val\_loss: 0.3667
Epoch 16/30
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{3s} 2ms/step -
accuracy: 0.8884 - loss: 0.3093 - val\_accuracy: 0.8666 - val\_loss: 0.3629
Epoch 17/30
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{4s} 2ms/step -
accuracy: 0.8900 - loss: 0.3037 - val\_accuracy: 0.8674 - val\_loss: 0.3609
Epoch 18/30
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{3s} 2ms/step -
accuracy: 0.8918 - loss: 0.2984 - val\_accuracy: 0.8678 - val\_loss: 0.3598
Epoch 19/30
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{3s} 2ms/step -
accuracy: 0.8938 - loss: 0.2934 - val\_accuracy: 0.8686 - val\_loss: 0.3590
Epoch 20/30
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{3s} 2ms/step -
accuracy: 0.8958 - loss: 0.2885 - val\_accuracy: 0.8686 - val\_loss: 0.3580
Epoch 21/30
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{3s} 2ms/step -
accuracy: 0.8979 - loss: 0.2838 - val\_accuracy: 0.8700 - val\_loss: 0.3593
Epoch 22/30
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{3s} 2ms/step -
accuracy: 0.9001 - loss: 0.2792 - val\_accuracy: 0.8694 - val\_loss: 0.3567
Epoch 23/30
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{4s} 2ms/step -
accuracy: 0.9014 - loss: 0.2748 - val\_accuracy: 0.8704 - val\_loss: 0.3590
Epoch 24/30
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{3s} 2ms/step -
accuracy: 0.9030 - loss: 0.2707 - val\_accuracy: 0.8722 - val\_loss: 0.3579
Epoch 25/30
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{3s} 2ms/step -
accuracy: 0.9044 - loss: 0.2666 - val\_accuracy: 0.8722 - val\_loss: 0.3559
Epoch 26/30
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{3s} 2ms/step -
accuracy: 0.9056 - loss: 0.2627 - val\_accuracy: 0.8712 - val\_loss: 0.3577
Epoch 27/30
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{3s} 2ms/step -
accuracy: 0.9070 - loss: 0.2589 - val\_accuracy: 0.8710 - val\_loss: 0.3560
Epoch 28/30
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{3s} 2ms/step -
accuracy: 0.9086 - loss: 0.2552 - val\_accuracy: 0.8714 - val\_loss: 0.3571
Epoch 29/30
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{3s} 2ms/step -
accuracy: 0.9099 - loss: 0.2516 - val\_accuracy: 0.8722 - val\_loss: 0.3569
Epoch 30/30
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{3s} 2ms/step -
accuracy: 0.9115 - loss: 0.2480 - val\_accuracy: 0.8730 - val\_loss: 0.3553
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k+kn}{import} \PY{n}{classification\PYZus{}report}\PY{p}{,} \PY{n}{ConfusionMatrixDisplay}


\PY{k}{def} \PY{n+nf}{classStats}\PY{p}{(}\PY{n}{actuals}\PY{p}{,} \PY{n}{preds}\PY{p}{,} \PY{n}{classNames}\PY{p}{)}\PY{p}{:}
    \PY{n+nb}{print}\PY{p}{(}\PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{actuals}\PY{p}{,} \PY{n}{preds}\PY{p}{)}\PY{p}{)}
    \PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
    \PY{n}{ConfusionMatrixDisplay}\PY{o}{.}\PY{n}{from\PYZus{}predictions}\PY{p}{(}
        \PY{n}{actuals}\PY{p}{,} \PY{n}{preds}\PY{p}{,} \PY{n}{display\PYZus{}labels}\PY{o}{=}\PY{n}{classNames}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{ax}
    \PY{p}{)}


\PY{n}{preds} \PY{o}{=} \PY{n+nb+bp}{cls}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{XTest}\PY{p}{)}
\PY{n}{preds} \PY{o}{=} \PY{n}{preds}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}

\PY{n}{classStats}\PY{p}{(}\PY{n}{yTest}\PY{p}{,} \PY{n}{preds}\PY{p}{,} \PY{n}{classNames}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{313/313} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{1s} 2ms/step
              precision    recall  f1-score   support

           0       0.84      0.82      0.83      1000
           1       0.99      0.96      0.98      1000
           2       0.75      0.83      0.79      1000
           3       0.80      0.93      0.86      1000
           4       0.79      0.80      0.79      1000
           5       0.89      0.98      0.93      1000
           6       0.78      0.57      0.66      1000
           7       0.95      0.90      0.92      1000
           8       0.94      0.97      0.95      1000
           9       0.97      0.94      0.95      1000

    accuracy                           0.87     10000
   macro avg       0.87      0.87      0.87     10000
weighted avg       0.87      0.87      0.87     10000

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Neural Networks_files/Neural Networks_22_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
If the training data was very skewed, with over/under-representation of
some classes you can set the \texttt{class\_weight} argument in the
\texttt{fit} method to give a larger weight to underpresented classes
and a smaller weight to overrepresented classes. This is used when
computing loss during training, so the model gives more importance to
underrepresented classes. If per instance weights are needed the
\texttt{sample\_weight} argument can be used. When both
\texttt{class\_weight} and \texttt{sample\_weight} are used the weights
are multiplied. For example if you wanted to give more weight to more
recent data you could use \texttt{sample\_weight} and if you wanted to
give more weight to underrepresented classes you could use
\texttt{class\_weight}.

The \texttt{fit()} method returns a \texttt{History} object containing
the training parameters (\texttt{history.params}), the list of epochs it
went through (\texttt{history.epoch}) and a dictionary
(\texttt{history.history}) containing the loss and extra metrics it
measured at the end of each epoch on the training set and on the
validation set. You can use the \texttt{history.history} dictionary to
plot the learning curves.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{15}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}

\PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{hist}\PY{o}{.}\PY{n}{history}\PY{p}{)}\PY{o}{.}\PY{n}{plot}\PY{p}{(}
    \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,}
    \PY{n}{xlim}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{29}\PY{p}{]}\PY{p}{,}
    \PY{n}{ylim}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}
    \PY{n}{grid}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
    \PY{n}{xlabel}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Epoch}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
    \PY{n}{style}\PY{o}{=}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{r\PYZhy{}\PYZhy{}.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{r\PYZhy{}\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b\PYZhy{}*}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,}
\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{15}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<Axes: xlabel='Epoch'>
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Neural Networks_files/Neural Networks_24_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{16}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{hist} \PY{o}{=} \PY{n+nb+bp}{cls}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{XTrain}\PY{p}{,} \PY{n}{yTrain}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{XValid}\PY{p}{,} \PY{n}{yValid}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Epoch 1/5
\textbf{ 430/1719} \textcolor{ansi-green}{━━━━━}\textcolor{ansi-white}{━━━━━━━━━━━━━━━} \textbf{1s} 1ms/step -
accuracy: 0.9149 - loss: 0.2405
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{3s} 2ms/step -
accuracy: 0.9135 - loss: 0.2446 - val\_accuracy: 0.8724 - val\_loss: 0.3561
Epoch 2/5
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{3s} 2ms/step -
accuracy: 0.9147 - loss: 0.2414 - val\_accuracy: 0.8734 - val\_loss: 0.3570
Epoch 3/5
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{3s} 1ms/step -
accuracy: 0.9158 - loss: 0.2381 - val\_accuracy: 0.8736 - val\_loss: 0.3571
Epoch 4/5
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{3s} 2ms/step -
accuracy: 0.9167 - loss: 0.2350 - val\_accuracy: 0.8730 - val\_loss: 0.3584
Epoch 5/5
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{3s} 1ms/step -
accuracy: 0.9178 - loss: 0.2319 - val\_accuracy: 0.8734 - val\_loss: 0.3590
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{17}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{preds} \PY{o}{=} \PY{n+nb+bp}{cls}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{XTest}\PY{p}{)}
\PY{n}{preds} \PY{o}{=} \PY{n}{preds}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}

\PY{n}{display}\PY{p}{(}\PY{n+nb+bp}{cls}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{XTest}\PY{p}{,} \PY{n}{yTest}\PY{p}{)}\PY{p}{)}
\PY{n}{classStats}\PY{p}{(}\PY{n}{yTest}\PY{p}{,} \PY{n}{preds}\PY{p}{,} \PY{n}{classNames}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{313/313} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{1s} 2ms/step
\textbf{313/313} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 884us/step -
accuracy: 0.8727 - loss: 0.3713
    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
[0.3727381229400635, 0.8705000281333923]
    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
              precision    recall  f1-score   support

           0       0.84      0.81      0.83      1000
           1       0.99      0.96      0.98      1000
           2       0.76      0.84      0.79      1000
           3       0.81      0.92      0.86      1000
           4       0.79      0.82      0.80      1000
           5       0.88      0.98      0.93      1000
           6       0.79      0.57      0.66      1000
           7       0.95      0.89      0.92      1000
           8       0.94      0.97      0.96      1000
           9       0.97      0.94      0.95      1000

    accuracy                           0.87     10000
   macro avg       0.87      0.87      0.87     10000
weighted avg       0.87      0.87      0.87     10000

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Neural Networks_files/Neural Networks_26_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{18}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{hist}\PY{o}{.}\PY{n}{history}\PY{p}{)}\PY{o}{.}\PY{n}{plot}\PY{p}{(}
    \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,}
    \PY{n}{xlim}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{]}\PY{p}{,}
    \PY{n}{ylim}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}
    \PY{n}{grid}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
    \PY{n}{xlabel}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Epoch}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
    \PY{n}{style}\PY{o}{=}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{r\PYZhy{}\PYZhy{}.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{r\PYZhy{}\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b\PYZhy{}*}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,}
\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{18}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<Axes: xlabel='Epoch'>
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Neural Networks_files/Neural Networks_27_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
\subsection{Building a Regression MLP using the Sequential
API}\label{building-a-regression-mlp-using-the-sequential-api}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{19}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{utils}\PY{o}{.}\PY{n}{set\PYZus{}random\PYZus{}seed}\PY{p}{(}\PY{l+m+mi}{42}\PY{p}{)}

\PY{n}{housing} \PY{o}{=} \PY{n}{fetch\PYZus{}california\PYZus{}housing}\PY{p}{(}\PY{p}{)}
\PY{n}{XTrainFull}\PY{p}{,} \PY{n}{XTest}\PY{p}{,} \PY{n}{yTrainFull}\PY{p}{,} \PY{n}{yTest} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}
    \PY{n}{housing}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{n}{housing}\PY{o}{.}\PY{n}{target}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}
\PY{p}{)}
\PY{n}{XTrain}\PY{p}{,} \PY{n}{XValid}\PY{p}{,} \PY{n}{yTrain}\PY{p}{,} \PY{n}{yValid} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}
    \PY{n}{XTrainFull}\PY{p}{,} \PY{n}{yTrainFull}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}
\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{20}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{norm} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Normalization}\PY{p}{(}\PY{p}{)}

\PY{n}{reg} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}
    \PY{p}{[}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{n}{XTrain}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{,}
        \PY{n}{norm}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{relu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{relu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{relu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}
    \PY{p}{]}
\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{21}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{opt} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{optimizers}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}3}\PY{p}{)}

\PY{n}{reg}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{mse}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{n}{opt}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RootMeanSquaredError}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{22}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{reg}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{Model: "sequential\_2"}

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃\textbf{ }\textbf{Layer (type)                   }\textbf{ }┃\textbf{ }\textbf{Output Shape          }\textbf{ }┃\textbf{ }\textbf{      Param \#}\textbf{ }┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ normalization (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Normalization})   │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{8})              │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{17} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_6 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                 │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{50})             │           \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{450} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_7 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                 │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{50})             │         \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{2,550} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_8 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                 │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{50})             │         \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{2,550} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_9 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                 │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1})              │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{51} │
└─────────────────────────────────┴────────────────────────┴───────────────┘

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Total params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{5,618} (21.95 KB)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{5,601} (21.88 KB)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Non-trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{17} (72.00 B)

    \end{Verbatim}

    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{23}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{norm}\PY{o}{.}\PY{n}{adapt}\PY{p}{(}\PY{n}{XTrain}\PY{p}{)}
\PY{n}{hist} \PY{o}{=} \PY{n}{reg}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{XTrain}\PY{p}{,} \PY{n}{yTrain}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{XValid}\PY{p}{,} \PY{n}{yValid}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Epoch 1/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{1s} 1ms/step -
RootMeanSquaredError: 1.2143 - loss: 1.5762 - val\_RootMeanSquaredError: 0.6527 -
val\_loss: 0.4261
Epoch 2/20
\textbf{  1/363} \textcolor{ansi-white}{━━━━━━━━━━━━━━━━━━━━} \textbf{10s} 28ms/step -
RootMeanSquaredError: 0.8502 - loss: 0.7228
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{1s} 2ms/step -
RootMeanSquaredError: 0.6372 - loss: 0.4066 - val\_RootMeanSquaredError: 0.5933 -
val\_loss: 0.3521
Epoch 3/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step -
RootMeanSquaredError: 0.6074 - loss: 0.3692 - val\_RootMeanSquaredError: 0.6970 -
val\_loss: 0.4858
Epoch 4/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step -
RootMeanSquaredError: 0.5922 - loss: 0.3509 - val\_RootMeanSquaredError: 0.7863 -
val\_loss: 0.6183
Epoch 5/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 964us/step -
RootMeanSquaredError: 0.5808 - loss: 0.3375 - val\_RootMeanSquaredError: 1.1434 -
val\_loss: 1.3074
Epoch 6/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step -
RootMeanSquaredError: 0.5719 - loss: 0.3272 - val\_RootMeanSquaredError: 1.3506 -
val\_loss: 1.8242
Epoch 7/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 998us/step -
RootMeanSquaredError: 0.5658 - loss: 0.3202 - val\_RootMeanSquaredError: 0.7185 -
val\_loss: 0.5162
Epoch 8/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 974us/step -
RootMeanSquaredError: 0.5564 - loss: 0.3096 - val\_RootMeanSquaredError: 0.8469 -
val\_loss: 0.7172
Epoch 9/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 958us/step -
RootMeanSquaredError: 0.5513 - loss: 0.3040 - val\_RootMeanSquaredError: 0.6418 -
val\_loss: 0.4119
Epoch 10/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 980us/step -
RootMeanSquaredError: 0.5463 - loss: 0.2985 - val\_RootMeanSquaredError: 0.6756 -
val\_loss: 0.4564
Epoch 11/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step -
RootMeanSquaredError: 0.5426 - loss: 0.2944 - val\_RootMeanSquaredError: 0.7295 -
val\_loss: 0.5321
Epoch 12/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 973us/step -
RootMeanSquaredError: 0.5396 - loss: 0.2912 - val\_RootMeanSquaredError: 1.0084 -
val\_loss: 1.0169
Epoch 13/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 961us/step -
RootMeanSquaredError: 0.5377 - loss: 0.2891 - val\_RootMeanSquaredError: 0.6520 -
val\_loss: 0.4251
Epoch 14/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 970us/step -
RootMeanSquaredError: 0.5341 - loss: 0.2853 - val\_RootMeanSquaredError: 1.0415 -
val\_loss: 1.0847
Epoch 15/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 945us/step -
RootMeanSquaredError: 0.5318 - loss: 0.2828 - val\_RootMeanSquaredError: 0.7899 -
val\_loss: 0.6239
Epoch 16/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 957us/step -
RootMeanSquaredError: 0.5298 - loss: 0.2807 - val\_RootMeanSquaredError: 1.1863 -
val\_loss: 1.4073
Epoch 17/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 990us/step -
RootMeanSquaredError: 0.5282 - loss: 0.2790 - val\_RootMeanSquaredError: 0.5334 -
val\_loss: 0.2845
Epoch 18/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step -
RootMeanSquaredError: 0.5241 - loss: 0.2748 - val\_RootMeanSquaredError: 0.5660 -
val\_loss: 0.3204
Epoch 19/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 998us/step -
RootMeanSquaredError: 0.5215 - loss: 0.2720 - val\_RootMeanSquaredError: 0.5181 -
val\_loss: 0.2684
Epoch 20/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 979us/step -
RootMeanSquaredError: 0.5194 - loss: 0.2698 - val\_RootMeanSquaredError: 0.5378 -
val\_loss: 0.2893
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{24}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{mse}\PY{p}{,} \PY{n}{rmse} \PY{o}{=} \PY{n}{reg}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{XTest}\PY{p}{,} \PY{n}{yTest}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{162/162} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step -
RootMeanSquaredError: 0.5235 - loss: 0.2742
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{25}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{XNew} \PY{o}{=} \PY{n}{XTest}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}
\PY{n}{preds} \PY{o}{=} \PY{n}{reg}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{XNew}\PY{p}{)}
\PY{n}{preds}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{1/1} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 92ms/step
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{25}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
array([[0.47],
       [1.2 ],
       [4.8 ]], dtype=float32)
\end{Verbatim}
\end{tcolorbox}
        
\subsection{Building Complex Models using the Functional
API}\label{building-complex-models-using-the-functional-api}

An example of a non-sequential neural network is a \emph{Wide \& Deep}
neural network, where all or part of the inputs are connected directly
to the output layer. This makes it possible for the network to learn
both deep patterns (using the deep path) and simple rules (through the
short path). In contrast a regular MLP forces all the data to flow
through the deep path, thus simple patterns in the data may end up being
distorted by this sequence of transformations.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{26}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{functools} \PY{k+kn}{import} \PY{n}{partial}
\PY{k+kn}{from} \PY{n+nn}{tensorflow}\PY{n+nn}{.}\PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers} \PY{k+kn}{import} \PY{n}{Normalization}\PY{p}{,} \PY{n}{Dense}\PY{p}{,} \PY{n}{Concatenate}\PY{p}{,} \PY{n}{Input}

\PY{n}{hidLay} \PY{o}{=} \PY{n}{partial}\PY{p}{(}\PY{n}{Dense}\PY{p}{,} \PY{n}{units}\PY{o}{=}\PY{l+m+mi}{30}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{relu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{normLayer} \PY{o}{=} \PY{n}{Normalization}\PY{p}{(}\PY{p}{)}
\PY{n}{hidLay1} \PY{o}{=} \PY{n}{hidLay}\PY{p}{(}\PY{p}{)}
\PY{n}{hidLay2} \PY{o}{=} \PY{n}{hidLay}\PY{p}{(}\PY{p}{)}
\PY{n}{concatLayer} \PY{o}{=} \PY{n}{Concatenate}\PY{p}{(}\PY{p}{)}
\PY{n}{outputLayer} \PY{o}{=} \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}

\PY{n}{input\PYZus{}} \PY{o}{=} \PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{n}{XTrain}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}
\PY{n}{normed} \PY{o}{=} \PY{n}{normLayer}\PY{p}{(}\PY{n}{input\PYZus{}}\PY{p}{)}
\PY{n}{hid1} \PY{o}{=} \PY{n}{hidLay1}\PY{p}{(}\PY{n}{normed}\PY{p}{)}
\PY{n}{hid2} \PY{o}{=} \PY{n}{hidLay2}\PY{p}{(}\PY{n}{hid1}\PY{p}{)}
\PY{n}{concat} \PY{o}{=} \PY{n}{concatLayer}\PY{p}{(}\PY{p}{[}\PY{n}{normed}\PY{p}{,} \PY{n}{hid2}\PY{p}{]}\PY{p}{)}
\PY{n}{output} \PY{o}{=} \PY{n}{outputLayer}\PY{p}{(}\PY{n}{concat}\PY{p}{)}

\PY{n}{regWide} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{Model}\PY{p}{(}\PY{n}{inputs}\PY{o}{=}\PY{p}{[}\PY{n}{input\PYZus{}}\PY{p}{]}\PY{p}{,} \PY{n}{outputs}\PY{o}{=}\PY{p}{[}\PY{n}{output}\PY{p}{]}\PY{p}{)}
\PY{n}{regWide}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{Model: "functional\_7"}

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃\textbf{ }\textbf{Layer (type)       }\textbf{ }┃\textbf{ }\textbf{Output Shape     }\textbf{ }┃\textbf{ }\textbf{   Param \#}\textbf{ }┃\textbf{ }\textbf{Connected to     }\textbf{ }┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ input\_layer\_3       │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{8})         │          \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0} │ -                 │
│ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{InputLayer})        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ normalization\_1     │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{8})         │         \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{17} │ input\_layer\_3[\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0}]… │
│ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Normalization})     │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense\_10 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})    │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{30})        │        \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{270} │ normalization\_1[\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{…} │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense\_11 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})    │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{30})        │        \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{930} │ dense\_10[\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0}][\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0}]    │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate         │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{38})        │          \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0} │ normalization\_1[\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{…} │
│ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Concatenate})       │                   │            │ dense\_11[\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0}][\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0}]    │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense\_12 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})    │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1})         │         \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{39} │ concatenate[\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0}][\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0}] │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Total params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1,256} (4.91 KB)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1,239} (4.84 KB)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Non-trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{17} (72.00 B)

    \end{Verbatim}

    
\begin{itemize}
\item
  First we create five layers: a \texttt{Normalization} layer to
  standardize inputs, two \texttt{Dense} layers with 30 neurons each
  using the \(\text{ReLU}\) activation function, a \texttt{Concatenate}
  layer, and one more \texttt{Dense} layer with a single output layer
  without any activation function.
\item
  Next we create an \texttt{Input} object, specifying the shape and
  dtype of the inputs. This is used as a function, passing it the
  inputs.
\item
  Then we pass the \texttt{Normalization} layer just like a function,
  passing the \texttt{Input} layer. The passing of layers as arguments
  to other layers only serves to connect them together.
\item
  We pass \texttt{normed} to \texttt{hidLay1}, which outputs
  \texttt{hid1} and we pass \texttt{hid1} to \texttt{hidLay2}, which
  outputs \texttt{hid2}.
\item
  So far all the connections have been sequential, but then we use
  \texttt{concatLayer} to concatenate the input and second hidden
  layer's output
\item
  Then pass \texttt{concat} to the \texttt{outputLayer} which finally
  gives us \texttt{output}.
\item
  Finally we create a Keras model, specifying which inputs and outputs
  to use.
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{27}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{opt} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{optimizers}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}3}\PY{p}{)}
\PY{n}{regWide}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{mse}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{n}{opt}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RootMeanSquaredError}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{28}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{normLayer}\PY{o}{.}\PY{n}{adapt}\PY{p}{(}\PY{n}{XTrain}\PY{p}{)}
\PY{n}{hist} \PY{o}{=} \PY{n}{regWide}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{XTrain}\PY{p}{,} \PY{n}{yTrain}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{XValid}\PY{p}{,} \PY{n}{yValid}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Epoch 1/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{1s} 1ms/step -
RootMeanSquaredError: 1.4678 - loss: 2.2793 - val\_RootMeanSquaredError: 2.9178 -
val\_loss: 8.5133
Epoch 2/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 844us/step -
RootMeanSquaredError: 0.7139 - loss: 0.5107 - val\_RootMeanSquaredError: 1.7047 -
val\_loss: 2.9059
Epoch 3/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step -
RootMeanSquaredError: 0.6398 - loss: 0.4096 - val\_RootMeanSquaredError: 1.0166 -
val\_loss: 1.0334
Epoch 4/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 902us/step -
RootMeanSquaredError: 0.6173 - loss: 0.3812 - val\_RootMeanSquaredError: 0.6176 -
val\_loss: 0.3815
Epoch 5/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 952us/step -
RootMeanSquaredError: 0.6050 - loss: 0.3662 - val\_RootMeanSquaredError: 0.5938 -
val\_loss: 0.3526
Epoch 6/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 876us/step -
RootMeanSquaredError: 0.5967 - loss: 0.3562 - val\_RootMeanSquaredError: 0.5811 -
val\_loss: 0.3377
Epoch 7/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 887us/step -
RootMeanSquaredError: 0.5902 - loss: 0.3485 - val\_RootMeanSquaredError: 0.5746 -
val\_loss: 0.3301
Epoch 8/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 857us/step -
RootMeanSquaredError: 0.5838 - loss: 0.3409 - val\_RootMeanSquaredError: 0.5665 -
val\_loss: 0.3209
Epoch 9/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 861us/step -
RootMeanSquaredError: 0.5788 - loss: 0.3351 - val\_RootMeanSquaredError: 0.5610 -
val\_loss: 0.3147
Epoch 10/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step -
RootMeanSquaredError: 0.5745 - loss: 0.3302 - val\_RootMeanSquaredError: 0.5579 -
val\_loss: 0.3113
Epoch 11/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 901us/step -
RootMeanSquaredError: 0.5712 - loss: 0.3264 - val\_RootMeanSquaredError: 0.5627 -
val\_loss: 0.3166
Epoch 12/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 891us/step -
RootMeanSquaredError: 0.5684 - loss: 0.3232 - val\_RootMeanSquaredError: 0.5619 -
val\_loss: 0.3158
Epoch 13/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 859us/step -
RootMeanSquaredError: 0.5652 - loss: 0.3196 - val\_RootMeanSquaredError: 0.6016 -
val\_loss: 0.3620
Epoch 14/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 864us/step -
RootMeanSquaredError: 0.5624 - loss: 0.3164 - val\_RootMeanSquaredError: 0.5646 -
val\_loss: 0.3187
Epoch 15/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step -
RootMeanSquaredError: 0.5596 - loss: 0.3133 - val\_RootMeanSquaredError: 0.7858 -
val\_loss: 0.6175
Epoch 16/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 958us/step -
RootMeanSquaredError: 0.5574 - loss: 0.3108 - val\_RootMeanSquaredError: 0.6477 -
val\_loss: 0.4196
Epoch 17/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 939us/step -
RootMeanSquaredError: 0.5553 - loss: 0.3084 - val\_RootMeanSquaredError: 1.1061 -
val\_loss: 1.2234
Epoch 18/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 901us/step -
RootMeanSquaredError: 0.5533 - loss: 0.3063 - val\_RootMeanSquaredError: 0.6701 -
val\_loss: 0.4490
Epoch 19/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 916us/step -
RootMeanSquaredError: 0.5516 - loss: 0.3044 - val\_RootMeanSquaredError: 1.7290 -
val\_loss: 2.9895
Epoch 20/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 900us/step -
RootMeanSquaredError: 0.5539 - loss: 0.3069 - val\_RootMeanSquaredError: 1.9701 -
val\_loss: 3.8811
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{29}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{regWide}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{XTest}\PY{p}{,} \PY{n}{yTest}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{162/162} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 2ms/step -
RootMeanSquaredError: 0.5631 - loss: 0.3174
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{29}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
[0.31699109077453613, 0.5630196332931519]
\end{Verbatim}
\end{tcolorbox}
        
In passing different subsets of features along different paths in the
network multiple inputs could be used.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{30}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{inputWide} \PY{o}{=} \PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{,} \PY{n}{name}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{inputWide}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{inputDeep} \PY{o}{=} \PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{6}\PY{p}{]}\PY{p}{,} \PY{n}{name}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{inputDeep}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{normLayerWide} \PY{o}{=} \PY{n}{Normalization}\PY{p}{(}\PY{p}{)}
\PY{n}{normLayerDeep} \PY{o}{=} \PY{n}{Normalization}\PY{p}{(}\PY{p}{)}

\PY{n}{normWide} \PY{o}{=} \PY{n}{normLayerWide}\PY{p}{(}\PY{n}{inputWide}\PY{p}{)}
\PY{n}{normDeep} \PY{o}{=} \PY{n}{normLayerDeep}\PY{p}{(}\PY{n}{inputDeep}\PY{p}{)}

\PY{n}{hid1} \PY{o}{=} \PY{n}{hidLay}\PY{p}{(}\PY{p}{)}\PY{p}{(}\PY{n}{normDeep}\PY{p}{)}
\PY{n}{hid2} \PY{o}{=} \PY{n}{hidLay}\PY{p}{(}\PY{p}{)}\PY{p}{(}\PY{n}{hid1}\PY{p}{)}

\PY{n}{concat} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{p}{[}\PY{n}{normWide}\PY{p}{,} \PY{n}{hid2}\PY{p}{]}\PY{p}{)}

\PY{n}{output} \PY{o}{=} \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{(}\PY{n}{concat}\PY{p}{)}

\PY{n}{regWide} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{Model}\PY{p}{(}\PY{n}{inputs}\PY{o}{=}\PY{p}{[}\PY{n}{inputWide}\PY{p}{,} \PY{n}{inputDeep}\PY{p}{]}\PY{p}{,} \PY{n}{outputs}\PY{o}{=}\PY{p}{[}\PY{n}{output}\PY{p}{]}\PY{p}{)}
\PY{n}{regWide}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{Model: "functional\_9"}

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃\textbf{ }\textbf{Layer (type)       }\textbf{ }┃\textbf{ }\textbf{Output Shape     }\textbf{ }┃\textbf{ }\textbf{   Param \#}\textbf{ }┃\textbf{ }\textbf{Connected to     }\textbf{ }┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ inputDeep           │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{6})         │          \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0} │ -                 │
│ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{InputLayer})        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ normalization\_3     │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{6})         │         \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{13} │ inputDeep[\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0}][\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0}]   │
│ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Normalization})     │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ inputWide           │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{5})         │          \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0} │ -                 │
│ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{InputLayer})        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense\_13 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})    │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{30})        │        \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{210} │ normalization\_3[\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{…} │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ normalization\_2     │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{5})         │         \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{11} │ inputWide[\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0}][\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0}]   │
│ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Normalization})     │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense\_14 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})    │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{30})        │        \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{930} │ dense\_13[\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0}][\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0}]    │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate\_1       │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{35})        │          \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0} │ normalization\_2[\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{…} │
│ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Concatenate})       │                   │            │ dense\_14[\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0}][\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0}]    │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense\_15 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})    │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1})         │         \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{36} │ concatenate\_1[\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0}]… │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Total params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1,200} (4.70 KB)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1,176} (4.59 KB)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Non-trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{24} (104.00 B)

    \end{Verbatim}

    
\begin{itemize}
\item
  Each \texttt{Dense} layer is created and called on the same line as is
  common practice. However this isn't done with the
  \texttt{Normalization} layers as a reference to the layer is needed
  for adaptation.
\item
  Used \texttt{tf.keras.layers.concatenate} to concatenate the inputs
  and the output of the second hidden layer.
\item
  We specified two inputs when creating the model as there are two
  inputs.
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{31}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{opt} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{optimizers}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}3}\PY{p}{)}
\PY{n}{regWide}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{mse}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{n}{opt}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RootMeanSquaredError}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{32}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{XTrainWide}\PY{p}{,} \PY{n}{XTrainDeep} \PY{o}{=} \PY{n}{XTrain}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{,} \PY{n}{XTrain}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{:}\PY{p}{]}
\PY{n}{XValidWide}\PY{p}{,} \PY{n}{XValidDeep} \PY{o}{=} \PY{n}{XValid}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{,} \PY{n}{XValid}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{:}\PY{p}{]}
\PY{n}{XTestWide}\PY{p}{,} \PY{n}{XTestDeep} \PY{o}{=} \PY{n}{XTest}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{,} \PY{n}{XTest}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{:}\PY{p}{]}
\PY{n}{XNewWide}\PY{p}{,} \PY{n}{XNewDeep} \PY{o}{=} \PY{n}{XTestWide}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{,} \PY{n}{XTestDeep}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}

\PY{n}{normLayerWide}\PY{o}{.}\PY{n}{adapt}\PY{p}{(}\PY{n}{XTrainWide}\PY{p}{)}
\PY{n}{normLayerDeep}\PY{o}{.}\PY{n}{adapt}\PY{p}{(}\PY{n}{XTrainDeep}\PY{p}{)}

\PY{n}{hist} \PY{o}{=} \PY{n}{regWide}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
    \PY{p}{(}\PY{n}{XTrainWide}\PY{p}{,} \PY{n}{XTrainDeep}\PY{p}{)}\PY{p}{,}
    \PY{n}{yTrain}\PY{p}{,}
    \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{p}{(}\PY{n}{XValidWide}\PY{p}{,} \PY{n}{XValidDeep}\PY{p}{)}\PY{p}{,} \PY{n}{yValid}\PY{p}{)}\PY{p}{,}
    \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,}
\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Epoch 1/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{1s} 1ms/step -
RootMeanSquaredError: 1.7786 - loss: 3.2631 - val\_RootMeanSquaredError: 0.9088 -
val\_loss: 0.8260
Epoch 2/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 917us/step -
RootMeanSquaredError: 0.8502 - loss: 0.7241 - val\_RootMeanSquaredError: 0.9169 -
val\_loss: 0.8408
Epoch 3/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 940us/step -
RootMeanSquaredError: 0.7640 - loss: 0.5841 - val\_RootMeanSquaredError: 0.8662 -
val\_loss: 0.7503
Epoch 4/20
\textbf{297/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━}\textcolor{ansi-white}{━━━━} \textbf{0s} 679us/step -
RootMeanSquaredError: 0.7173 - loss: 0.5149
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{1s} 1ms/step -
RootMeanSquaredError: 0.7125 - loss: 0.5081 - val\_RootMeanSquaredError: 0.9910 -
val\_loss: 0.9821
Epoch 5/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step -
RootMeanSquaredError: 0.6711 - loss: 0.4506 - val\_RootMeanSquaredError: 0.9600 -
val\_loss: 0.9216
Epoch 6/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step -
RootMeanSquaredError: 0.6402 - loss: 0.4101 - val\_RootMeanSquaredError: 0.8299 -
val\_loss: 0.6888
Epoch 7/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 938us/step -
RootMeanSquaredError: 0.6181 - loss: 0.3821 - val\_RootMeanSquaredError: 1.3156 -
val\_loss: 1.7309
Epoch 8/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 954us/step -
RootMeanSquaredError: 0.6084 - loss: 0.3702 - val\_RootMeanSquaredError: 1.4480 -
val\_loss: 2.0968
Epoch 9/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step -
RootMeanSquaredError: 0.6010 - loss: 0.3612 - val\_RootMeanSquaredError: 1.2405 -
val\_loss: 1.5388
Epoch 10/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 929us/step -
RootMeanSquaredError: 0.5949 - loss: 0.3541 - val\_RootMeanSquaredError: 1.3897 -
val\_loss: 1.9313
Epoch 11/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 926us/step -
RootMeanSquaredError: 0.5920 - loss: 0.3506 - val\_RootMeanSquaredError: 1.3386 -
val\_loss: 1.7919
Epoch 12/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 939us/step -
RootMeanSquaredError: 0.5907 - loss: 0.3490 - val\_RootMeanSquaredError: 1.2488 -
val\_loss: 1.5596
Epoch 13/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 940us/step -
RootMeanSquaredError: 0.5863 - loss: 0.3438 - val\_RootMeanSquaredError: 0.9414 -
val\_loss: 0.8862
Epoch 14/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 936us/step -
RootMeanSquaredError: 0.5836 - loss: 0.3407 - val\_RootMeanSquaredError: 0.9483 -
val\_loss: 0.8992
Epoch 15/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 896us/step -
RootMeanSquaredError: 0.5806 - loss: 0.3372 - val\_RootMeanSquaredError: 0.9576 -
val\_loss: 0.9170
Epoch 16/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 899us/step -
RootMeanSquaredError: 0.5799 - loss: 0.3364 - val\_RootMeanSquaredError: 1.0987 -
val\_loss: 1.2072
Epoch 17/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 922us/step -
RootMeanSquaredError: 0.5794 - loss: 0.3357 - val\_RootMeanSquaredError: 1.0939 -
val\_loss: 1.1966
Epoch 18/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 974us/step -
RootMeanSquaredError: 0.5778 - loss: 0.3339 - val\_RootMeanSquaredError: 1.2056 -
val\_loss: 1.4534
Epoch 19/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 965us/step -
RootMeanSquaredError: 0.5776 - loss: 0.3337 - val\_RootMeanSquaredError: 1.1802 -
val\_loss: 1.3928
Epoch 20/20
\textbf{363/363} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step -
RootMeanSquaredError: 0.5764 - loss: 0.3323 - val\_RootMeanSquaredError: 1.2530 -
val\_loss: 1.5700
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{33}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{mseTest} \PY{o}{=} \PY{n}{regWide}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{p}{(}\PY{n}{XTestWide}\PY{p}{,} \PY{n}{XTestDeep}\PY{p}{)}\PY{p}{,} \PY{n}{yTest}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{162/162} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step -
RootMeanSquaredError: 0.5803 - loss: 0.3369
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{34}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{preds} \PY{o}{=} \PY{n}{regWide}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{p}{(}\PY{n}{XNewWide}\PY{p}{,} \PY{n}{XNewDeep}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{1/1} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 59ms/step
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]

\end{Verbatim}
\end{tcolorbox}

\chapter{Deep Computer Vision and Convolutional Neural Networks
(CNNs)}\label{deep-computer-vision-and-convolutional-neural-networks-cnns}

Convolutional Neural Networks are designed based on the way the human
visual cortex perceives images with the neurons in the visual cortex
being sensitive to small sub-regions of the visual field, called
\emph{receptive fields}. This is the basis of the design of CNNs, where
the neurons in the first layer are connected to the pixels in the images
in their receptive fields, and the neurons in the next layer are
connected to the neurons in the previous layer, and so on

\section{Convolutional Layers}\label{convolutional-layers}

Neurons in the first Convolutional layer are not connected to every
pixel in the input image but instead only to pixels in their receptive
fields. In turn each neuron in the second convolutional layer is
connected only to neurons located within in a small rectangle in the
first layer. This architecture allows the network to concentrate on
low-level features in the first hidden layer, then assemble them into
higher-level features in the next hidden layer, and so on. The layers of
a CNN are arranged in 2 dimensions in rows and in columns unlike the
multi-layered perceptrons which are arranged in a single dimension.

A neuron located in row \(i\), column \(j\) of a given layer is
connected to the outputs of the neurons in the previous layer located in
rows \(i\) to \(i + f_h - 1\), columns \(j\) to \(j + f_w - 1\), where
\(f_h\) and \(f_w\) are the height and width of the receptive field. In
order for a layer to have the same height and weight as the previous
layer it is common to add zeros around the inputs, called \emph{zero
padding}. It is also possible to connect a large input layer to a much
smaller layer by spacing out the receptive fields, thereby greatly
reducing the model's computational complexity.

The horizontal or vertical step size from one receptive field to the
next is called the \emph{stride}. By increasing the stride the output
layer will be smaller than the input layer, but the receptive field will
be larger.

\subsection{Filters}\label{filters}

A neuron's weights can be represented as a small image the size of the
receptive field. These weights can be termed as \emph{filters} /
\emph{convolutional kernels} / \emph{kernels}. These weights can be used
to change what data the neuron actually takes in with entires in the
filter being the parameters that the backpropagation algorithm can
tweak. For example for a neuron with a receptive field of size 7x7, a
filter could be a 7x7 matrix with all entries being 0 except for the 4th
column which would be 1s. This filter would cause the neuron to detect
everything in its receptive field except for the central vertical line,
making it only sensitive to vertical lines.

Now if all the neurons in a layer use this vertical line filter (and the
same bias term) an image passed to this layer would enhance the vertical
lines in the image, while the horizontal lines would be blurred. Thus a
layer full of neuron using the same filter outputs a \textbf{feature
map}, which highlights the areas in an image that activate the filter
the most. During training the convolutional layer learns the most useful
features fo the task at hand, which above layers can then use to detect
more complex patterns.

Employing the filter of a feature map the output of a neuron in a
convolutional layer can be calculated as follows: \[
z_{i, j} = b + \displaystyle\sum_{u=0}^{f_h-1} \displaystyle\sum_{v=0}^{f_w-1} x_{i', j'} \times w_{u, v} \text{ with } \begin{cases}
i' = i \times s_h + u \\
j' = j \times s_w + v
\end{cases}
\]

With:

\begin{itemize}
\item
  \(z_{i,j}\) being the output of the neuron located in row \(i\) and
  column \(j\) in the convolutional layer
\item
  \(b\) being the bias term for the neuron
\item
  \(s_h\) and \(s_w\) being the vertical and horizontal strides
\item
  \(f_h\) and \(f_w\) being the height and width of the receptive field
\item
  \(x_{i', j'}\) being the input of the neuron located in row \(i'\) and
  column \(j'\) in the previous layer
\item
  \(w_{u,v}\) being the connection weight between any neuron in the
  previous layer and the neuron located in row \(u\) and column \$v in
  the convolutional layer
\end{itemize}

\subsection{Stacking Feature Maps}\label{stacking-feature-maps}

Convolution layers can have multiple filters and outputs one feature map
per feature. CLs have one on neuron per pixel in each feature map, and
all neurons within a given feature map share the same parameters, kernel
and bias term. Neurons in different feature maps have different
parameters, meaning the previous CL in the network neurons' receptive
fields extend across the depth of the next CL, simultaneously applying
all the filters to its input making it able to detect multiple features
anywhere in its inputs.

Input layers are also composed of layers on per colour channel, usually
coloured pictures have Red, Green and Blue layers. Specifically a neuron
in the \(i\)th row, \(j\)th column and \(k\) feature map of a CL \(l\)
is connected to the output neurons of the \(l-1\) CL in the in the rows
\(i \times s_h\) to \(i \times s_h + f_h - 1\) and columns
\(j \times s_w\) to \(j \times s_w + f_w - 1\), across all feature maps
in later \(l-1\). Note that within a CL, all the neuron in the same row
\(i\) and column \(j\) but in different feature maps are connected to
the outputs of the same neurons in the previous layer. This leads to the
output of a neuron in a feature map having the following equation: \[
z_{i,j,k} = b_k + \displaystyle\sum_{u = 0}^{f_h - 1} \displaystyle\sum_{v = 0}^{f_w - 1} \displaystyle\sum_{k' = 0}^{f_{n'} - 1} x_{i', j', k'} \times w_{u, v, k', k} \text{ with } \begin{cases}
i' = i \times s_h + u \\
j' = j \times s_w + v
\end{cases}
\]

With

\begin{itemize}
\item
  \(z_{i, j, k}\) being the output of the neuron located in row \(i\),
  column \(j\) in feature map \(k\) of the convolutional layer \(l\)
\item
  \(s_h\) and \(s_h\) being the vertical and horizontal stride and
  \(f_w\) and \(f_h\) being the width and height of the receptive field
\item
  \(f_{n'}\) being the number of feature maps in the previous layer
  (\(l - 1\))
\item
  \(x_{i', j', k'}\) being the output if the neuron located in layer
  \(l-1\), row \(i'\), column \(j'\) and feature map \(k'\) or channel
  \(k'\) if the previous layer is the input layer.
\item
  \(b_k\) being the bias term for the feature map \(k\) in layer \(l\),
  which basically determines the brightness of feature map \(k\)
\item
  \(w_{u, v, k', k}\) being the connection weight between any neuron in
  feature map \(k\) of the layer \(l\) and its input located at row
  \(u\), column \(v\), relative to the neuron's receptive field, and
  feature map \(k'\)
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{35}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{datasets} \PY{k+kn}{import} \PY{n}{load\PYZus{}sample\PYZus{}images}

\PY{n}{images} \PY{o}{=} \PY{n}{load\PYZus{}sample\PYZus{}images}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{images}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\PY{n}{images} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{CenterCrop}\PY{p}{(}\PY{n}{height}\PY{o}{=}\PY{l+m+mi}{70}\PY{p}{,} \PY{n}{width}\PY{o}{=}\PY{l+m+mi}{120}\PY{p}{)}\PY{p}{(}\PY{n}{images}\PY{p}{)}
\PY{n}{images} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Rescaling}\PY{p}{(}\PY{n}{scale}\PY{o}{=}\PY{l+m+mi}{1} \PY{o}{/} \PY{l+m+mi}{255}\PY{p}{)}\PY{p}{(}\PY{n}{images}\PY{p}{)}
\PY{n}{images}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{35}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
TensorShape([2, 70, 120, 3])
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{36}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{convLayer1} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Conv2D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{7}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{same}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{fmaps} \PY{o}{=} \PY{n}{convLayer1}\PY{p}{(}\PY{n}{images}\PY{p}{)}
\PY{n}{fmaps}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{36}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
TensorShape([2, 70, 120, 32])
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{37}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{kernels}\PY{p}{,} \PY{n}{biases} \PY{o}{=} \PY{n}{convLayer1}\PY{o}{.}\PY{n}{get\PYZus{}weights}\PY{p}{(}\PY{p}{)}
\PY{n}{kernels}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{37}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
(7, 7, 3, 32)
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{38}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{biases}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{38}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
(32,)
\end{Verbatim}
\end{tcolorbox}
        
\subsection{Activation functions}\label{activation-functions}

As the convolutional layers perform linear operations, and thus can only
learn linear transformations of the input data, it is necessary to
introduce non-linearity to the layers to allow the network to learn
complex patterns. This is done by applying an activation function to the
output of each neuron in the convolutional layer. The most common
activation function used in CNNs is the \textbf{Rectified Linear Unit
(ReLU)}, already defined above. This function is usually applied to the
hidden convolutional layers of the network, but not to the output layer.

Activation functions are applied on the whole layer at once, meaning
that the same activation function is applied to all the neurons in the
layer. This is done to ensure that the network can learn to detect
different patterns in different parts of the image. Therefore the
activation function augments the output of the neuron in the feature map
\(k\) of the layer \(l\) as follows: \[
h_{i, j, k} = \text{ReLU}(z_{i, j, k})
\]

And in the case of a convolutional layer with a single feature map: \[
h_{i, j} = \text{ReLU}(z_{i, j})
\]

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{39}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{convLayer1} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Conv2D}\PY{p}{(}
    \PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{7}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{same}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{relu}\PY{l+s+s2}{\PYZdq{}}
\PY{p}{)}
\PY{n}{fmaps} \PY{o}{=} \PY{n}{convLayer1}\PY{p}{(}\PY{n}{images}\PY{p}{)}
\PY{n}{fmaps}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{39}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
TensorShape([2, 70, 120, 32])
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{40}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{kernels}\PY{p}{,} \PY{n}{biases} \PY{o}{=} \PY{n}{convLayer1}\PY{o}{.}\PY{n}{get\PYZus{}weights}\PY{p}{(}\PY{p}{)}
\PY{n}{kernels}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{40}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
(7, 7, 3, 32)
\end{Verbatim}
\end{tcolorbox}
        
\section{Pooling Layers}\label{pooling-layers}

The goal of pooling layers is to subsample / shrink the input image in
order to reduce the computational load, memory usage and the number of
parameters. Just like convolutional layers each neuron in a pooling
layer is only connected to a limited number of neurons in the previous
layer in a receptive field, however a pooling layer has no weights, all
it does is aggregate the inputs using an aggregation function such as
the mean or the max. A pooling layer employing the max aggregation
function is called a \textbf{max pooling layer}.

Other than reducing computations, memory usage, and the number of
parameters, max pooling layers introduce some level of
\textbf{\emph{invariance}} to small translations. This means that if a
small feature is slightly moved in the input image, the output of the
max pooling layer may not change. This can be useful for detecting
patterns in images, as the location of the pattern in the image will not
matter. By inserting a max pooling layer after every convolutional
layer, the network can learn to be invariant to small translations which
can be useful for classification tasks.

Max Pooling is however very destructive, as it discards all the
information except for the maximum value. This can be a problem for
tasks that require precise localization. To address this issue, a common
strategy is to add a convolutional layer with the same number of filters
as the max pooling layer just before the max pooling layer. This way the
network can learn to preserve the information that will be most useful
to the max pooling layer.

Pooling can also be performed along the depth dimension instead of
spatial dimensions which will allow the CNN to be invariant to various
features, such as rotation and scaling.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{41}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{maxPool} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{MaxPool2D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{42}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{DepthPool}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Layer}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{o}{*}\PY{o}{*}\PY{n}{kwargs}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{kwargs}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{pool\PYZus{}size} \PY{o}{=} \PY{n}{pool\PYZus{}size}

    \PY{k}{def} \PY{n+nf}{call}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{inputs}\PY{p}{)}\PY{p}{:}
        \PY{n}{shape} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{shape}\PY{p}{(}\PY{n}{inputs}\PY{p}{)}
        \PY{n}{groups} \PY{o}{=} \PY{n}{shape}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{/}\PY{o}{/} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{pool\PYZus{}size}
        \PY{n}{newShape} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{shape}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{n}{groups}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{pool\PYZus{}size}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
        \PY{k}{return} \PY{n}{tf}\PY{o}{.}\PY{n}{reduce\PYZus{}max}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{inputs}\PY{p}{,} \PY{n}{newShape}\PY{p}{)}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

Another type of pooling layer used commonly is the \emph{global average
pooling layer}. This layer computes the mean of each entire feature map,
like an average pooling layer using a pooling size of the entire feature
map. This means the layer outputs a single number per feature map and
per instance. Although this is very destructive it can be useful just
before the output layer.

Keras does not have a Depth wise Max Pooling layer, but it can be
implemented reshaping its inputs to split the channels into groups of
the desired, size then using \texttt{tf.reduce\_max()} to compute the
max of each group. This implementation assumes that the stride is equal
to the pooling size.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{43}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{globAvgPool} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{GlobalAveragePooling2D}\PY{p}{(}\PY{p}{)}
\PY{n}{globAvgPool}\PY{p}{(}\PY{n}{images}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{43}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<tf.Tensor: shape=(2, 3), dtype=float32, numpy=
array([[0.64338624, 0.5971759 , 0.5824972 ],
       [0.76306933, 0.2601113 , 0.10849128]], dtype=float32)>
\end{Verbatim}
\end{tcolorbox}
        
\section{CNN Architectures}\label{cnn-architectures}

A typical CNN architecture consists of a few convolutional layers, each
generally followed by a ReLU layer, then a pooling layer, then another
few convolutional layers again followed by a ReLU, then another pooling
layer, and so on. The image gets smaller and smaller as it goes through
the network, but also getting deeper and deeper due to the increasing
number of filters in each convolutional layer. At the top of the stack
of convolutional layers a regular feedforward neural network is added,
composed of a few fully connected layers (+ReLU), and the final layer
outputs the prediction, potentially using a softmax activation function
if the network is used for classification to output estimated class
probabilities.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{44}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{functools} \PY{k+kn}{import} \PY{n}{partial}

\PY{n}{conv2DLayer} \PY{o}{=} \PY{n}{partial}\PY{p}{(}
    \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Conv2D}\PY{p}{,}
    \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,}
    \PY{n}{padding}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{same}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
    \PY{n}{activation}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{relu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
    \PY{n}{kernel\PYZus{}initializer}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{he\PYZus{}normal}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
\PY{p}{)}
\PY{n}{ffLayer} \PY{o}{=} \PY{n}{partial}\PY{p}{(}
    \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{relu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{kernel\PYZus{}initializer}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{he\PYZus{}normal}\PY{l+s+s2}{\PYZdq{}}
\PY{p}{)}

\PY{n}{cnnCls} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}
    \PY{p}{[}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{28}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Reshape}\PY{p}{(}\PY{n}{target\PYZus{}shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{28}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{,}
        \PY{n}{conv2DLayer}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{7}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{MaxPool2D}\PY{p}{(}\PY{p}{)}\PY{p}{,}
        \PY{n}{conv2DLayer}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{128}\PY{p}{)}\PY{p}{,}
        \PY{n}{conv2DLayer}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{128}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{MaxPool2D}\PY{p}{(}\PY{p}{)}\PY{p}{,}
        \PY{n}{conv2DLayer}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{)}\PY{p}{,}
        \PY{n}{conv2DLayer}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{MaxPool2D}\PY{p}{(}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Flatten}\PY{p}{(}\PY{p}{)}\PY{p}{,}
        \PY{n}{ffLayer}\PY{p}{(}\PY{n}{units}\PY{o}{=}\PY{l+m+mi}{128}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{,}
        \PY{n}{ffLayer}\PY{p}{(}\PY{n}{units}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{,}
        \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{n}{units}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{softmax}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{,}
    \PY{p}{]}
\PY{p}{)}

\PY{n}{cnnCls}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{Model: "sequential\_3"}

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃\textbf{ }\textbf{Layer (type)                   }\textbf{ }┃\textbf{ }\textbf{Output Shape          }\textbf{ }┃\textbf{ }\textbf{      Param \#}\textbf{ }┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ reshape (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Reshape})               │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{28}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{28}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1})      │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d\_2 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Conv2D})               │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{28}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{28}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{64})     │         \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{3,200} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max\_pooling2d\_1 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{MaxPooling2D})  │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{14}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{14}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{64})     │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d\_3 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Conv2D})               │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{14}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{14}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{128})    │        \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{73,856} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d\_4 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Conv2D})               │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{14}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{14}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{128})    │       \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{147,584} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max\_pooling2d\_2 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{MaxPooling2D})  │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{7}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{7}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{128})      │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d\_5 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Conv2D})               │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{7}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{7}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{256})      │       \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{295,168} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d\_6 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Conv2D})               │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{7}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{7}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{256})      │       \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{590,080} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max\_pooling2d\_3 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{MaxPooling2D})  │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{3}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{3}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{256})      │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ flatten\_2 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Flatten})             │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{2304})           │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_16 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{128})            │       \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{295,040} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dropout (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dropout})               │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{128})            │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_17 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{64})             │         \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{8,256} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dropout\_1 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dropout})             │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{64})             │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_18 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{10})             │           \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{650} │
└─────────────────────────────────┴────────────────────────┴───────────────┘

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Total params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1,413,834} (5.39 MB)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1,413,834} (5.39 MB)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Non-trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0} (0.00 B)

    \end{Verbatim}

    
\begin{itemize}
\item
  First we use the \texttt{partial()} function to defined a template
  Convolutional Layer, with a small kernel size of 3x3, a stride of 1,
  the \texttt{same} padding, and the ReLU activation function. This
  template can be used to create any number of layers in the CNN.
\item
  Next we create the CNN with the first \texttt{Input} layer taking in
  images in array representation of size \(28\times 28\times 1\) as the
  images in the Fashion MNIST dataset are of size \(28\times 28\) and
  are in greyscale, with a \texttt{Reshape} layer to ensure the images
  are of this size.
\item
  The first CL is of size 64 with fairly large filters (\(7\times 7\))
  which creates a 64 feature maps each of size \(28\times 28\).
\item
  Then we add a max pooling layer with a default pool size of 2, so it
  downscales the feature maps by a factor of 2, resulting in 64 feature
  maps of size \(14\times 14\).
\item
  We repeat the same structure twice, two convolutional layers followed
  by a max pooling layer, each adding on more feature maps, deepening
  the network. For larger images this structure could be repeated using
  the number of repetitions hyperparameter. The number of filters double
  as we climb up the CNN toward the output layer (64, 128, 256), as the
  number of low-level features is usually much lower than the number of
  high-level features. It is common to double the number of filters
  after each pooling layer as the pooling layer reduces the size of the
  feature maps by 2 by default, so we can afford to double the number of
  filters in the next layer without increasing the computational load.
\item
  Next is the fully connected network, composed of two dense hidden
  layers and a dense output layer. Since it is a classification task
  with 10 classes the output layer has 10 layers and uses the softmax
  activation function to determine the class probabilities of each
  class. We must flatten the inputs just before the first dense layer as
  it expects a 1D array of features for each instance. Two dropout
  layers with a dropout rate of 50\% each have also been added to reduce
  overfitting.
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{45}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{cnnCls}\PY{o}{.}\PY{n}{compile}\PY{p}{(}
    \PY{n}{loss}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{sparse\PYZus{}categorical\PYZus{}crossentropy}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
    \PY{n}{optimizer}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{optimizers}\PY{o}{.}\PY{n}{SGD}\PY{p}{(}\PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{0.05}\PY{p}{)}\PY{p}{,}
    \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{accuracy}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,}
\PY{p}{)}
\PY{n}{cnnCls}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{Model: "sequential\_3"}

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃\textbf{ }\textbf{Layer (type)                   }\textbf{ }┃\textbf{ }\textbf{Output Shape          }\textbf{ }┃\textbf{ }\textbf{      Param \#}\textbf{ }┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ reshape (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Reshape})               │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{28}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{28}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1})      │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d\_2 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Conv2D})               │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{28}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{28}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{64})     │         \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{3,200} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max\_pooling2d\_1 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{MaxPooling2D})  │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{14}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{14}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{64})     │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d\_3 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Conv2D})               │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{14}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{14}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{128})    │        \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{73,856} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d\_4 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Conv2D})               │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{14}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{14}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{128})    │       \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{147,584} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max\_pooling2d\_2 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{MaxPooling2D})  │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{7}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{7}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{128})      │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d\_5 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Conv2D})               │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{7}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{7}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{256})      │       \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{295,168} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d\_6 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Conv2D})               │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{7}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{7}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{256})      │       \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{590,080} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max\_pooling2d\_3 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{MaxPooling2D})  │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{3}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{3}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{256})      │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ flatten\_2 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Flatten})             │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{2304})           │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_16 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{128})            │       \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{295,040} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dropout (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dropout})               │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{128})            │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_17 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{64})             │         \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{8,256} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dropout\_1 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dropout})             │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{64})             │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_18 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{10})             │           \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{650} │
└─────────────────────────────────┴────────────────────────┴───────────────┘

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Total params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1,413,834} (5.39 MB)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1,413,834} (5.39 MB)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Non-trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{0} (0.00 B)

    \end{Verbatim}

    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{46}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{p}{(}\PY{n}{XTrainFull}\PY{p}{,} \PY{n}{yTrainFull}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{n}{XTest}\PY{p}{,} \PY{n}{yTest}\PY{p}{)} \PY{o}{=} \PY{n}{fmnist}

\PY{n}{cutoff} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{5000}
\PY{n}{XTrain}\PY{p}{,} \PY{n}{yTrain} \PY{o}{=} \PY{n}{XTrainFull}\PY{p}{[}\PY{p}{:}\PY{n}{cutoff}\PY{p}{]}\PY{p}{,} \PY{n}{yTrainFull}\PY{p}{[}\PY{p}{:}\PY{n}{cutoff}\PY{p}{]}
\PY{n}{XValid}\PY{p}{,} \PY{n}{yValid} \PY{o}{=} \PY{n}{XTrainFull}\PY{p}{[}\PY{n}{cutoff}\PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{yTrainFull}\PY{p}{[}\PY{n}{cutoff}\PY{p}{:}\PY{p}{]}
\PY{n}{XTrain}\PY{p}{,} \PY{n}{XValid}\PY{p}{,} \PY{n}{XTest} \PY{o}{=} \PY{n}{XTrain} \PY{o}{/} \PY{l+m+mf}{255.0}\PY{p}{,} \PY{n}{XValid} \PY{o}{/} \PY{l+m+mf}{255.0}\PY{p}{,} \PY{n}{XTest} \PY{o}{/} \PY{l+m+mf}{255.0}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{47}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{hist} \PY{o}{=} \PY{n}{cnnCls}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{XTrain}\PY{p}{,} \PY{n}{yTrain}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{7}\PY{p}{,} \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{XValid}\PY{p}{,} \PY{n}{yValid}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Epoch 1/7
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{172s} 99ms/step
- accuracy: 0.5581 - loss: 1.2113 - val\_accuracy: 0.8534 - val\_loss: 0.4111
Epoch 2/7
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{161s} 94ms/step
- accuracy: 0.8238 - loss: 0.5049 - val\_accuracy: 0.8718 - val\_loss: 0.3413
Epoch 3/7
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{169s} 98ms/step
- accuracy: 0.8605 - loss: 0.4066 - val\_accuracy: 0.8874 - val\_loss: 0.3043
Epoch 4/7
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{166s} 97ms/step
- accuracy: 0.8781 - loss: 0.3537 - val\_accuracy: 0.8954 - val\_loss: 0.2863
Epoch 5/7
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{165s} 96ms/step
- accuracy: 0.8951 - loss: 0.3128 - val\_accuracy: 0.9036 - val\_loss: 0.2639
Epoch 6/7
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{163s} 95ms/step
- accuracy: 0.9034 - loss: 0.2843 - val\_accuracy: 0.9080 - val\_loss: 0.2556
Epoch 7/7
\textbf{1719/1719} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{167s} 97ms/step
- accuracy: 0.9125 - loss: 0.2618 - val\_accuracy: 0.9050 - val\_loss: 0.2645
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{48}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{preds} \PY{o}{=} \PY{n}{cnnCls}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{XTest}\PY{p}{)}
\PY{n}{preds} \PY{o}{=} \PY{n}{preds}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{cnnCls}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{XTest}\PY{p}{,} \PY{n}{yTest}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{313/313} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{10s} 32ms/step
\textbf{313/313} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{10s} 32ms/step -
accuracy: 0.9029 - loss: 0.3004
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{48}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
[0.29452764987945557, 0.9016000032424927]
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{49}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{classStats}\PY{p}{(}\PY{n}{yTest}\PY{p}{,} \PY{n}{preds}\PY{p}{,} \PY{n}{classNames}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
              precision    recall  f1-score   support

           0       0.86      0.85      0.86      1000
           1       0.99      0.98      0.99      1000
           2       0.79      0.88      0.83      1000
           3       0.86      0.96      0.90      1000
           4       0.85      0.80      0.83      1000
           5       0.93      0.99      0.96      1000
           6       0.80      0.68      0.74      1000
           7       0.96      0.93      0.95      1000
           8       0.99      0.98      0.99      1000
           9       0.99      0.95      0.97      1000

    accuracy                           0.90     10000
   macro avg       0.90      0.90      0.90     10000
weighted avg       0.90      0.90      0.90     10000

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Neural Networks_files/Neural Networks_71_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{50}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{utils}\PY{o}{.}\PY{n}{plot\PYZus{}model}\PY{p}{(}\PY{n}{cnnCls}\PY{p}{,} \PY{n}{show\PYZus{}shapes}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{dpi}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}
 
            
\prompt{Out}{outcolor}{50}{}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Neural Networks_files/Neural Networks_72_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]

\end{Verbatim}
\end{tcolorbox}


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
