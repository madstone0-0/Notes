\documentclass[12pt letter]{report}
\input{./template/preamble}
\input{./template/macros}
\input{./template/letterfonts}

\title{\Huge{Probability}}
\author{\huge{Madiba Hudson-Quansah}}
\date{}
\usepackage{parskip}
\setcounter{tocdepth}{5}
\setcounter{secnumdepth}{5}

\begin{document}
\maketitle
\newpage
\pdfbookmark[section]{\contentsname}{too}
\tableofcontents
\pagebreak

\chapter{Module 8: Introduction}

\section{Introduction To Probability}

\dfn{Probability}{
	A mathematical description of randomness and uncertainty / The likelihood of an event occurring.

	The notation for Probability is $\mathbb{P} \left( X \right) $ where $X$ is the event.

	Probability is always between $0 \leq \mathbb{P} \left( X \right) \leq 1$ or $0\% \leq \mathbb{P} \left( X \right) \leq 100\%$.
}

There are two ways of determining probability:

\begin{itemize}
	\item Theoretical / Classical - Determined by the nature of the experiment
	\item Empirical / Observational - Determined by the results of the experiment
\end{itemize}

\section{Relative Frequency}

\dfn{Relative Frequency}{
	Relative frequency is the number of times an event occurs divided by the total number of trials.

	\[
		\mathbb{P} \left( X \right) = \frac{\text{Number of times event occurs}}{\text{Total number of trials}}
	\]
}

\thm{The Law of Large Numbers}{
	As the number of trials increases, the relative frequency of an event approaches the theoretical probability of the event.
}


\chapter{Module 9: Find the Probability of Events}

\section{Sample Spaces and Events}

\dfn{Random Experiment}{
	An experiment whose outcome is determined by chance.
}

\dfn{Sample Space}{
	The list of possible outcomes of a random experiment, denoted by $S$.
}

\dfn{Event}{
	A statement about the nature of the outcome after the experiment has been conducted, denoted by any capital letter
	except $S$.
}

\section{Equally Likely Outcomes}

\[
	\mathbb{P} \left( A \right) = \frac{\text{Number of outcomes in }A}{\text{Number of outcomes in } S}
\]

Where $A$ is an event and $S$ is the sample space.

\section{Probability Rules}

\subsection{Rule 1: Probability is a Number Between 0 and 1}

For any event $A$, $0 \le \mathbb{P} \left( A \right) \le 1$.


\subsection{Rule 2: Addition Rule}

$\mathbb{P} \left( S \right) = 1 $, that is the sum of the probabilities of all possible outcomes is 1.

\subsection{Rule 3: Complement Rule}

$\mathbb{P} \left( A' \right) = 1 - \mathbb{P} \left( A \right)  $, that is the probability of the complement of an event is 1 minus the
probability the event occurs.


\subsection{Rule 4: Addition Rule for Mutually Exclusive Events}

\dfn{Mutually Exclusive / Disjoint events}{
	Events that cannot happen at the same time.

}

$\mathbb{P} \left( A \text{ or } B \right) = \mathbb{P} \left( \text{ event } A \text{ occurs or event } B \text{ occurs
		or both occur} \right)  $\\


\noindent If $A$ and $B$ are mutually exclusive, then $\mathbb{P} \left( A \text{ or } B \right) = \mathbb{P} \left( A
	\right) + \mathbb{P} \left( B \right)   $

\subsection{Rule 5: Multiplication Rule for Independent Events}
$\mathbb{P} \left( A \text{ and } B \right) = \mathbb{P} \left( \text{ event } A \text{ occurs and event } B \text{
		occurs } \right)  $

\dfn{Independent Events}{
	Two events $A$ and $B$ are said to be independent if the occurrence of one event does not affect the probability of the
	other event occurring.

}

\dfn{Dependent Events}{
	Two events $A$ and $B$ are said to be dependent if the occurrence of one event affects the probability of the other event
	occurring.

}

\noindent If $A$ and $B$ are two independent events, then $\mathbb{P} \left( A \text{ and } B \right) = \mathbb{P} \left( A
	\right)  \times \mathbb{P} \left( B \right)  $

\subsection{Rule 6: General Addition Rule}

For any two events $A$ and $B$, $\mathbb{P} \left( A \text{ or } B \right) = \mathbb{P} \left( A \right) + \mathbb{P}
	\left( B \right) - \mathbb{P} \left( A \text{ and } B \right)    $. If the event are mutually exclusive, then
$\mathbb{P} \left( A \text{ and } B \right) = 0 $, giving us $\mathbb{P} \left( A \text{ or } B \right) =
	\mathbb{P}\left( A \right) + \mathbb{P} \left( B \right)   $, i.e. the addition rule for mutually exclusive events.


\chapter{Module 10: Conditional Probability and Independence}

\dfn{Conditional Probability}{
	The probability an event occurs as a result of another event. I.e. Probability of event $B$, given event event $A$
	is,

	\[
		\mathbb{P} \left( B  \mid  A \right)  = \frac{\mathbb{P} \left( A \text{ and } B \right) }{P \left( A \right) }
	\]
}

\section{Independence}

When two events are independent, the probability of one event occurring does not affect the probability of the other
event, i.e.

\begin{align*}
	\mathbb{P} \left( B  \mid  A \right) = \mathbb{P} \left( B \right)           \\
	\mathbb{P} \left( A  \mid B \right)  = \mathbb{P} \left( A \right)           \\
	\mathbb{P} \left( B  \mid A \right)  = \mathbb{P} \left( B  \mid A'  \right) \\
	\mathbb{P} \left( A \text{ and } B \right) = \mathbb{P} \left( A \right)  \times \mathbb{P} \left( B \right)
\end{align*}

\section{The General Multiplication Rule}

For any two dependent events $A$ and $B$
\[
	\mathbb{P} \left( A \text{ and } B \right) = \mathbb{P} \left( A \right) \times \mathbb{P} \left( B  \mid A \right)
\]

\section{Probability Trees}

\dfn{Probability Tree}{
	A diagram that shows the sample space of a random experiment and the probability of each outcome.
}

\subsection{Bayes' Theorem}

\[
	\mathbb{P} \left( A  \mid B \right)  = \frac{\mathbb{P} \left( A \right) \times P \left( B  \mid A \right)   }{
		\mathbb{P} \left( A \right) \times \mathbb{P} \left( B  \mid A \right) + \mathbb{P} \left( A' \right) \times \mathbb{P}
		\left( B  \mid A' \right) }
\]

\chapter{Module 11: Random Variables}

\dfn{Random Variable}{
	Assigns a unique numerical value to the outcome of a random experiment.
}

\dfn{Discrete Random Variable}{
	A random variable that can take on a finite number of values. Discrete random variables are usually counts.
}

\dfn{Continuous Random Variable}{
	A random variable that can take on an infinite number of values. Continuous random variables are usually measurements.

}

\section{Discrete Random Variables}

\subsection{Notation}

For a given event $X$, the probability of $X$ is denoted by $\mathbb{P} \left( X \right) $. For a given value $x$, the
probability of $X$ is denoted by $\mathbb{P} \left( X = x \right) $, i.e. the probability that $X$ takes on the value $x$.

\subsection{Probability Distribution}

\dfn{Probability Distribution}{
	The list of all possible values of a random variable and their corresponding probabilities.
}

Any probability distribution must satisfy the following two conditions:

\begin{itemize}
	\item $0 \leq \mathbb{P} \left( X = x  \right) \leq 1 $ - The probability of any value of $X$ is between 0 and 1.
	\item $\Sigma_x \,  \mathbb{P} \left( X = x \right) = 1 $ - The sum of the probabilities of all possible values of $X$ is 1.
\end{itemize}

\subsection{Key Words}

\begin{itemize}
	\item At least / No less than - $ x \geq$
	\item At most / No more than - $x \leq$
	\item Less than / fewer than - $ x <$
	\item  More than / greater than - $ x >$
	\item Exactly - $ x =$
\end{itemize}

\subsection{Mean and Variance of a Discrete Random Variable}

\subsubsection{Mean}

\dfn{Mean / Expected value of a Discrete Random Variable}{
	The average value of a random variable, denoted by $\mu$.
}

For a given random variable $X$, the mean is given by

\[
	\mu_{X} = \Sigma_{i=1}^{n} x_i p_i
\]

Where $x_i$ is the value of $X$ and $p_i$ is the probability of $X$ taking on the value $x_i$.


\paragraph{Applications of the Mean}

\begin{itemize}
	\item The mean of a random variable is the long-term average value of the random variable.
	\item The mean of a random variable is the centre of the probability distribution of the random variable.
\end{itemize}

\subsubsection{Variance}

\dfn{Variance}{
	The average of the squared differences between each value of a random variable and the mean of the random variable,
	denoted by $\sigma^2$.
}

For a given random variable $X$, the variance is given by
\[
	\sigma_{X}^2 = \Sigma_{i=1}^{n} \left( x_i - \mu_{X} \right)^2 p_i
\]
And standard deviation is given by
\[
	\sigma_{X} = \sqrt{\sigma_{X} ^2}
\]
Where $x_i$ is the value of $X$ and $p_i$ is the probability of $X$ taking on the value $x_i$.


\subsubsection{Rules for Mean and Variance of Random Discrete Variables}

\nparagraph{Adding or Subtracting a Constant to a Random Variable}

If $Y = X + c$, then $\mu_{Y} = \mu_{X} + c$, $\sigma_{Y}^2 = \sigma_{X}^2$ and $\sigma_{Y} = \sigma_{X}$. \\

If $Y = X - c$, then $\mu_{Y} = \mu_{X} - c$, $\sigma_{Y}^2 = \sigma_{X}^2$ and $\sigma_{Y} = \sigma_{X}$. \\

\nparagraph{Multiplying a Random Variable by a Constant $> 1$} \\
If $Y = cX\, , c > 1$, then $\mu_{Y} = c \mu_{X}$, $\sigma_{Y}^2 = c^2 \sigma_{X}^2$ and $\sigma_{Y} = c \sigma_{X}$ \\

\nparagraph{Multiplying a Random Variable by a Constant $< 1$}

If $Y = cX\, , c < 1$, then $\mu_{Y} = c \mu_{X}$, $\sigma_{Y}^2 = c^2 \sigma_{X}^2$ and $\sigma_{Y} = c \sigma_{X}$ \\

\nparagraph{Linear Transformation of a Random Variable}

If $Y = a + bX$, then $\mu_{Y} = a + b \mu_{X}$, $\sigma_{Y}^2 = b^2 \sigma_{X}^2$ and $\sigma_{Y} = \abs{b} \sigma_{X}$ \\

\nparagraph{Sum of Two Random Variables}

If $Z = X + Y$, then $\mu_{Z} = \mu_{X} + \mu_{Y}$, $\sigma_{Z}^2 = \sigma_{X}^2 + \sigma_{Y}^2$ and $\sigma_{Z} =
	\sqrt{\sigma_{X}^2 + \sigma_{Y}^2}$. Only if $X$ and $Y$ are independent. \\

\subsection{Binomial Random Variables}

\dfn{Binomial Random Variable}{
	A random variable that counts the number of successes in a fixed number of independent trials, denoted by $X \sim
		\text{Bin} \left( n, p \right) $.
	Where $n$ is the number of trials and $p$ is the probability of success.
}

\dfn{Binomial Experiment}{
	Random experiments that satisfy the following conditions:
	\begin{itemize}
		\item A fixed number of trials, denoted by $n$.
		\item Each trial is independent of the others.
		\item There are only two possible outcomes for each trial, success or failure.
		\item There is a constant probability of success, denoted by $p$, for each trial, which can be expressed as the
		      complement of the probability of failure, $q = 1 - p$.
	\end{itemize}

	\nt{
		The number $\left( X \right) $ of success in a sample of size $n$ taken without replacement from a population with
		proportion $p$ of successes is approximately binomial with $n$ and $p$ as long as the sample size is at most $10\%$
		of the population size $\left( N \right) $. I.e.
		\[
			n \leq 0.1N
		\]
		Or
		\[
			N \geq 10n
		\]
	}
}

To calculate the probability of a binomial random variable, we use the formula
\[
	\mathbb{P} \left( X = x \right) = \binom{n}{x} p^x q^{n-x}, \text{ where } x = 0, 1, 2, \ldots, n
\]

Where $n$ is the number of trials, $x$ is the number of successes, $p$ is the probability of success and $q$ is the
probability of failure. \\

If $X$ is Binomial with parameters $n$ and $p$, then
\[
	\mu_{X} = np
\]

And
\begin{align*}
	\sigma^2_{X} = np \left( 1 - p \right) \\
	\sigma_{X} = \sqrt{np \left( 1 - p \right) }
\end{align*}

\section{Continuous Random Variables}

\subsection{Probability Distribution}

For a continuous random variable $X$, the probability distribution is given by the \textit{probability density
	function}, whose properties are

\begin{itemize}
	\item $f \left( x \right) \geq 0$ for all $x$.
	\item $\int_{-\infty}^{\infty} f \left( x \right) \, dx = 1$
	\item The probability that $X$ takes on a value between $a$ and $b$ is given by
	      \[
		      \mathbb{P} \left( a \leq X \leq b \right) = \int_{a}^{b} f \left( x \right) \, dx
	      \]
\end{itemize}

\nt{
	\begin{itemize}
		\item The probability that a continuous random variable takes on a specific value is always 0.
		\item The strictness of the inequality does not matter, i.e. $\mathbb{P} \left( X \geq a \right) = \mathbb{P} \left( X > a \right)$
	\end{itemize}
}

\subsection{Normal Random Variables}

\dfn{Normal Random Variable}{
	A random variable that has a bell-shaped probability distribution, denoted by $X \sim N \left( \mu, \sigma^2 \right)
	$. Where $\mu $ is the mean and $\sigma^2$ is the variance.
}

For a normally distributed random variable $X$:

\begin{itemize}
	\item There is a $68\%$ chance that $X$ takes on a value within one standard deviation of the mean, i.e. $0.68 = \mathbb{P}
		      \left( \mu - \sigma < X < \mu + \sigma  \right) $
	\item There is a $95\%$ chance that $X$ takes on a value within two standard deviations of the mean, i.e. $0.95 = \mathbb{P}
		      \left( \mu - 2 \sigma < X < \mu + 2 \sigma  \right) $
	\item There is a $99.7\%$ chance that $X$ takes on a value within three standard deviations of the mean, i.e. $0.997 = \mathbb{P}
		      \left( \mu - 3 \sigma < X < \mu + 3 \sigma  \right) $
\end{itemize}

\subsubsection{Finding Probabilities for Normal Random Variables}

\nparagraph{Standardizing Values}

\dfn{$z$-score}{
	The number of standard deviations a value is from the mean of a normal random variable, denoted by $z$.

}

To standardize a normal random variable $X$, we must find its $z$-score, given by

\[
	z = \frac{x - \mu }{\sigma }
\]

\nparagraph{Finding Probabilities with the $z$-score}

\dfn{Normal Table}{
	A table that shows the probability that a standard normal random variable takes on a value less than a given $z$-score.
}

Using the $z$-score we can find the probability that a normal random variable takes on a value less than a given value
$x$, by tracing the $z$-score to the normal table.

\[
	\mathbb{P} \left( X < x \right) = \mathbb{P} \left( Z < z \right)
\]

On a standard normal table $z$-score are written to two decimal places as row headers and for additional precision the
column headers are the first two decimal places of the $z$-score.




\subsection{Uniform Distribution}

\dfn{Uniform Distribution}{
	,denoted by $X \sim U \left( a,b \right) $
}

For a random variable $X$, if is uniformly distributed over the interval $a$ and $b$ then its \textit{probability distribution
	density function} is given by
\[
	f \left( x \right)  = \begin{cases}
		\frac{1}{b - a}, \, a < x < b \\
		0, \, \text{otherwise}
	\end{cases}
\]

The mean, variance, and standard deviation of a uniformly distributed random variable is given by
\begin{align*}
	\mu_X = \frac{a + b}{2}                         \\
	\sigma^2_X = \frac{\left( b - a \right)^2 }{12} \\
	\sigma_X = \sqrt{\frac{\left( b - a \right)^2 }{12}}
\end{align*}

\chapter{Module 12: Sampling Distributions}

\section{Parameters vs. Statistics}

\dfn{Parameter}{
	A numerical value that describes a characteristic of a population, denoted by a Greek letter, e.g. $\mu, \sigma^2$.
}

\dfn{Statistic}{
	A numerical value that describes a characteristic of a sample, denoted by a Roman letter, e.g $\bar{x}, s^2$.

}

\dfn{Proportion}{
	A statistic that estimates the proportion of a population or sample that has a certain characteristic, denoted by
	$p$ for a population and $\hat{p}$ for a sample.
}

\dfn{Sampling Variability}{
	The variability of a statistic from one sample to another.
}

\section{Behaviour of Sample Proportion $\hat{p}$}

\subsection{Centre}

The mean of the sample proportion is the same as the population proportion, i.e.
\[
	\mu_{\hat{p}} = p
\]
As it is reasonable to expect all the sample proportions in repeated samples to average out to the underlying population.

\subsection{Spread}

The sample size has an effect on the spread of the distribution of the sample proportion, i.e. the \textbf{larger the sample
	size, the less spread out the distribution} of the sample proportion and \textbf{more spread for smaller sample
	sizes}. We can describe the spread of the distribution of the sample proportion more precisely by finding the actual
standard deviation of the sample proportion. i.e.
\[
	\sigma_{\hat{p}} = \sqrt{\frac{p \left( 1 - p \right) }{n}}
\]
Where $p$ is the population proportion and $n$ is the sample size.

\subsection{Shape}

The shape of the distribution of the sample proportion is approximately normal if the sample size is large enough. I.e.
if
\[
	np \geq 10 \text{ and } n \left( 1 - p \right) \geq 10
\]

\dfn{Sampling of Distribution of $\hat{p}$}{
	The distribution of the values of the sample proportions $\hat{p}$ in repeated samples.

}

\section{Behaviour of Sample Mean $\overline{X}$}

\subsection{Centre}

The mean of the sample mean is the same as the population mean, i.e.
\[
	\mu_{\overline{X}} = \mu
\]

\subsection{Spread}

The sample size has an effect on the spread of the distribution of the sample mean, i.e. the \textbf{larger the sample
	size, the less spread out the distribution} of the sample mean and \textbf{more spread for smaller sample sizes}. We can
describe the spread of the distribution of the sample mean more precisely by finding the actual standard deviation of the
sample mean. i.e.
\[
	\sigma_{\overline{X}} = \frac{\sigma}{\sqrt{n}}

\]

\subsection{Shape}

The shape of the distribution of the sample mean is approximately normal if the sample size is large enough. I.e. if
\[
	n \geq 30
\]

\dfn{Sampling of Distribution of $\overline{X}$}{
	The distribution of the values of the sample mean $\overline{X}$ in repeated samples.
}


\end{document}
