\documentclass[12pt letter]{report}
\input{./template/preamble}
\input{./template/macros}
\input{./template/letterfonts}

\title{\Huge{Inference}}
\author{\huge{Madiba Hudson-Quansah}}
\date{}
\usepackage{parskip}

\setcounter{tocdepth}{4}
\setcounter{secnumdepth}{4}

\begin{document}
\maketitle
\newpage
\pdfbookmark[section]{\contentsname}{too}
\tableofcontents
\pagebreak

\chapter{Module 13: Inference}

\dfn{Statistical Inference}{
  Inferring something about a population from a sample.
}

\dfn{Point Estimation}{
  Estimating an unknown parameter using a single number, calculated from the sample data.
}

\dfn{Interval Estimation}{
  Estimating an unknown parameter using an Interval of values that is likely to contain the true value of the
  parameter, and state how confident we are that the interval contains the true value.
}

\dfn{Hypothesis Testing}{
  Making decisions about the population parameter based on the sample data.
}

\section{Inference for One Variable}

Depending on the type of variable we are interested in the population parameter we infer about changes:
\begin{itemize}
  \item Categorical : Population Proportion $p$
  \item Quantitative : Population Mean $\mu$
\end{itemize}

\chapter{Module 14: Estimation}

\section{Point Estimation}

\dfn{Point Estimator}{
  A statistic that provides an estimate of a population parameter.
}

The point estimator also changes based on the type of variable examined:
\begin{itemize}
  \item Categorical: Sample Proportion / $\hat{p}$
  \item Quantitative: Sample Mean /  $\bar{x}$
\end{itemize}

\nt{
  The larger the sample size, the more accurate the point estimate.
}

\section{Interval Estimation}

\dfn{Confidence Interval}{
  An interval of values that is likely to contain the true value of the population parameter.
}

Interval estimation is based on the point estimate and the margin of error.

\subsection{Confidence Intervals for the Population Mean}

For a quantitative variable with a normally distributed sample mean distribution due to the Central Limit Theorem,
construing a 95\% confidence interval consists of the following steps:
\begin{itemize}
  \item Identify mean $\overline{X}$, which for a sample mean distribution is approximately equal to $\mu $
  \item Find the standard deviation $S$ of the sample mean distribution, $\frac{\sigma}{\sqrt{n}}$
  \item Find $\overline{X} \pm 2 S $, which are your upper and lower bounds of the confidence interval
\end{itemize}

Therefore generally the confidence interval is:
\[
  \overline{x} \pm 2 \times \frac{\sigma}{\sqrt{n}}
\]

\subsubsection{Other Levels of Confidence}

Constructing a 99\% confidence interval for $\mu $ can be done using:
\[
  \overline{x} \pm 2.576 \times \frac{\sigma}{\sqrt{n}}
\]

And a 90\% confidence interval for $\mu $ can be found using:
\[
  \overline{x} \pm 1.645 \times \frac{\sigma}{\sqrt{n}}
\]

To calculate the confidence interval for any level of confidence, we use the $z$-score of the area of half the $\alpha$ of the
confidence level, i.e.:
\[
  z^* = z_{\frac{\alpha}{2}}
\]

Where alpha is
\[
  \alpha = 1 - C
\]
Or
\[
  \alpha = 1 + C
\]
\subsection{General Structure of Confidence Intervals}

A confidence interval has the following form:
\[
  \overline{x} \pm z^{*} \times \frac{\sigma }{\sqrt{n}}
\]

Where $z^*$ is general notation for the multiplier that depends on the level of confidence. \\
The confidence interval can then also be expressed in the form:
\[
  \overline{x} \pm m
\]

Where $m = z^* \times \frac{\sigma}{\sqrt{n}}$ and $\overline{x}$ is the point estimator for the unknown population mean $\mu $\\
$m$ is called the margin of error, since it represents the maximum estimation error for a given level of confidence.

\nt{
  A larger sample size makes for a smaller margin of error.
}

\subsection{Sample Size Calculations}

The sample size required to estimate the population mean $\mu $ with a margin of error $m$ at a level of confidence $C$ can be found using:
\[
  n = \left( \frac{z^*  \sigma }{m} \right)^2
\]
Which is rounded up to the nearest whole number.

\subsection{When $\sigma $ is unknown}

When the population standard deviation $\sigma $ is unknown, the sample standard deviation $s$ is used instead, but as a
result we need to use a different set of confidence multipliers $t^*$, associated with the $t$ distribution. The
interval is therefore:
\[
  \overline{x} \pm t^* \times \frac{s}{\sqrt{n}}
\]

These multipliers depend not only on the level of confidence, but also on the sample size $n$. \\
For large values of $n$, the $t$ distribution approaches the standard normal distribution, and the $t^*$ multipliers,
therefore the $z^*$ multipliers can be used, i.e. $t^* \approx z^*$ and the confidence interval becomes:
\[
  \overline{x} \pm z^* \times \frac{s}{\sqrt{n}}
\]


\section{Confidence Intervals for the Population Proportion}

For a categorical variable, the population proportion $p$ can be estimated using the sample proportion $\hat{p}$, and the
margin of error $m$, i.e. the confidence interval is:
\[
  \hat{p} \pm m
\]

Where $m$ is:

\[
  m = z^* \times \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}
\]

Therefore:
\[
  \hat{p} \pm z^* \times \sqrt{\frac{\hat{p}\left( 1 - \hat{p} \right) }{n}}
\]

\subsection{Sample Size Calculations}

The sample size required to estimate the population proportion $p$ with a margin of error $m$ at a level of confidence
$C$ can be found using:

\[
  n = p \times \left( 1 - p \right) \times \left( \frac{z^*}{m} \right)^2
\]

\chapter{Module 15: Hypothesis Testing}

\section{Introduction}

\dfn{Hypothesis Testing}{
  Assessing evidence provided by the data in favour of against some claim about the population.
}

The process of statistical hypothesis testing is as follows:
\begin{itemize}
  \item We start with two claims about the behaviour of a population, where the claim usually contradict each other.
  \item Choose a sample and collect and summarize relevant data.
  \item Determine how likely it is to observe data, like the data we get had claim 1 been true
  \item Based on the results we make one of two conclusions:
        \begin{itemize}
          \item If we find that if claim 1 were true it would extremely unlikely to observe the data we observed, then we
                have strong evidence against claim 1 and can reject it in favour of claim 2
          \item If we find that if claim 1 were true it would not be extremely unlikely to observe the data we observed,
                then we do not have enough evidence against claim 1, and cannot reject it in favour of claim 2.
        \end{itemize}
\end{itemize}

In the terminology of hypothesis testing Claim 1 is termed as the \textbf{null hypothesis}, denoted by $H_0$, and Claim
2 is termed as the \textbf{alternative hypothesis}, denoted by $H_a$.

\begin{description}
  \item[Null Hypothesis] No change from the status quo / No relationship
  \item[Alternative Hypothesis] There is a change from the status quo / There is a relationship
\end{description}

Determining how likely it is to observe data like the data we would of gotten if claim 1 were true, is termed as
finding its \textit{$p$-value} \\

In making a decision about the null hypothesis, we use the $p$-value to determine the strength of the evidence against
the null hypothesis. The smaller the $p$-value, the stronger the evidence against the null hypothesis, i.e.:
\begin{itemize}
  \item If $p-\text{value} < \alpha \text{ (usually $0.5$)}$, we can reject $H_0$ and accept $H_{a}$, as the evidence
        against $H_0$ is strong.
  \item If $p-\text{value} > \alpha \text{ (usually $0.5$)}$, we do not have enough evidence against $H_0$ and cannot reject it.
\end{itemize}

\section{Hypothesis Testing for Population Proportion}

\subsection{Step 1 - Stating the Hypothesis}

In stating the null and alternative hypothesis for a population proportion, the null hypothesis always takes the form of
equality, and the alternative hypothesis takes the form of inequality / difference that can either be one-sided or
two-sided alternatives, where one-sided alternatives are used when we are interested in a specific direction of change,
and two-sided alternatives are used when the direction of change is irrelevant.
\begin{itemize}
  \item Null Hypothesis: $H_0: p = p_0$
  \item Alternative Hypothesis:
        \begin{itemize}
          \item $H_a: p \neq p_0$ (two-sided)
          \item $H_a: p > p_0$ (one-sided)
          \item $H_a: p < p_0$ (one-sided)
        \end{itemize}
\end{itemize}

\subsection{Step 2 - Collecting and Summarizing Data}

\section{Hypothesis Testing for Population Mean}

In hypothesis testing for a population mean there are two cases:
\begin{itemize}
  \item $\sigma $ is known and we use the $z$-test for the population mean $\mu $
  \item $\sigma $ is unknown and we use the $t$-test for the population mean $\mu $
\end{itemize}

\subsection{$z$-test for the Population Mean}

\subsubsection{Step 1 - Stating the Hypothesis}
The null and alternative hypothesis for the population mean $\mu $ takes the same form as the population mean,
i.e.:
\begin{itemize}
  \item Null Hypothesis: $H_0: \mu = \mu_0$
  \item Alternative Hypothesis:
        \begin{itemize}
          \item $H_a: \mu \neq \mu_0$ (two-sided)
          \item $H_a: \mu < \mu_0$ (one-sided)
          \item $H_a: \mu > \mu_0$ (one-sided)
        \end{itemize}
\end{itemize}
Where $\mu_0$ is the null value

\subsubsection{Step 2 - Collecting and Summarizing Data}

In this step we calculate the sample mean $\overline{x}$, and relevant sample statistic and summarize the data with a
test statistic. This test statistic is the $z$-score of the sample mean, assuming $H_0$ is true, i.e:
\[
  z = \frac{\overline{x} - \mu_0}{\frac{\sigma}{\sqrt{n}}}
\]
Where $\overline{x}$ is the sample mean, $\mu_0$ is the null value, $\sigma $ is the population standard deviation, And
$n$ is the sample size, where $n > 30$

The conditions needed to perform the $z$-test for the population mean are:
\begin{itemize}
  \item The sample is random
  \item The variable varies normally in the population
  \item The variable does not vary normally in the population, but the sample size is large enough
\end{itemize}

\subsubsection{Step 3 - Finding the $p$-value}

The $p$-value is the probability of observing a sample mean as extreme as the one observed, assuming $H_0$ is true.
The $p$-value is calculated using the $z$-score of the sample mean, i.e.:
\begin{itemize}
  \item $H_a: \mu \neq \mu_0 \implies p\text{-value} = 2 \mathbb{P}\left( Z \geq  \mid z \mid  \right) $
  \item $H_a: \mu < \mu_0 \implies  p\text{-value } = \mathbb{P}\left( Z \leq z \right) $
  \item $H_a: \mu > \mu_0 \implies p\text{-value} = \mathbb{P}\left( Z \geq z \right) $
\end{itemize}

\subsubsection{Step 4 - Drawing Conclusions}

In drawing conclusions about the null hypothesis, we compare the $p$-value to the level of significance $\alpha $, and
state our conclusion as follows:
\begin{itemize}
  \item $p\text{-value} > \alpha$ - We do not have enough evidence against $H_0$ and cannot reject it
  \item $p\text{-value} < \alpha$ - We have enough evidence against $H_0$ and can reject it in favour of $H_a$
\end{itemize}

\subsection{$t$-test for Population mean}

The first, second, and fourth steps in using the $t$-test for population mean are identical to that of the $z$-test, but the
$t$-score being calculated as:
\[
  t = \frac{\overline{x} - \mu_0}{\frac{s}{\sqrt{n}}}
\]
Where the denominator $\frac{s}{\sqrt{n}}$ is the standard error of $\overline{X}$. \\
This value is then compared to the $t$-distribution with $n-1$ degrees of freedom, and the $p$-value is calculated

\subsubsection{Step 3 - Finding the $p$-value}

The $p$-value is calculated using the $t$-score of the sample mean, i.e.:
\begin{itemize}
  \item $H_a: \mu \neq  \mu_0 \implies p\text{-value} = 2 \mathbb{P}\left( t \left( n - 1 \right) \geq  \mid t \mid   \right) $
  \item $H_a: \mu < \mu_0 \implies p\text{-value} = \mathbb{P}\left( t \left( n - 1 \right) \leq t  \right) $
  \item $H_a: \mu > \mu_0 \implies p\text{-value} = \mathbb{P}\left( t \left( n - 1 \right) \geq t  \right) $
\end{itemize}

\section{Type I and Type II Errors}

\subsection{Type I Error}
\dfn{Type I Error}{
  Rejecting the null hypothesis when it is true
}

\subsubsection{The Probability of Type I Error}

The probability of making a Type I error is denoted by $\alpha $, and is the level of significance of the test, i.e.:
\[
  \alpha = \mathbb{P}\left( \text{Type I Error} \right)
\]

\subsection{Type II Error}
\dfn{Type II Error}{
  Failing to reject the null hypothesis when it is false
}

\subsubsection{The Probability of a Type II Error}

The probability of making a Type II error is inversely related to the probability of making a Type I error, and is denoted by $\beta $, i.e.:
\[
  \beta = \mathbb{P}\left( \text{Type II Error} \right)
\]

\chapter{Module 16 and 17: Inference for Relationships}

\nt{
  Large Enough - $n > 30$
}

\section{Case $C \to Q$}

In this case, we are interested in the relationship between a categorical variable and a quantitative variable. To make
inferences about the relationship between the two variables, we compare the means of the quantitative variable across the
categories of the categorical variable. The method used depends on the number of categories of the categorical
variable., i.e:
\begin{itemize}
  \item $k = 2$: We use the two-sample $t$-test
  \item $k > 2$ We use the ANOVA test
\end{itemize}
Where $k$ is the number of categories of the categorical variable.

Furthermore when $k = 2$, there are two cases to consider:
\begin{itemize}
  \item Independent groups /samples - Where the two categories are independent of each other.
  \item Paired groups / Dependent samples - Where the two categories are dependent on each other in some way or matched pairs.
\end{itemize}

\subsection{Two sample $t$-test - Independent Groups}

\subsubsection{Step 1 - Stating the Hypothesis}

In this case the hypothesis represents the difference in the population means $\left( \mu_1 \text{ and } \mu_2  \right)
$ of the two categories of the categorical variable.i.e.
\begin{itemize}
  \item Null Hypothesis: $H_0: \mu_1 = \mu_2$ / $H_0: \mu_1 - \mu_2 = 0$
  \item Alternative Hypothesis:
        \begin{itemize}
          \item $H_a: \mu_1 - \mu_2 \neq 0$ (two-sided)
          \item $H_a: \mu_1 - \mu_2 > 0$ (one-sided)
          \item $H_a: \mu_1 - \mu_2 < 0$ (one-sided)
        \end{itemize}
\end{itemize}

\subsubsection{Step 2 - Collecting and Summarizing Data}
In this step we calculate the test statistic, which is in this case the $t$-score of the difference in the sample means,
i.e.:
\[
  t = \frac{\left( \overline{y_1} - \overline{y_2} \right) - 0 }{\sqrt{\frac{s^2_1}{n_1} + \frac{s^2_2}{n_2}}}
\]
Where:

$\overline{y_1}$ and $\overline{y_2}$ are the sample means of the samples from populations 1 and 2 respectively.

$s_1$ and $s_2$ are the sample standard deviations of the samples from populations 1 and 2 respectively.

$n_1$ and $n_2$ are the sample sizes of the two samples.
\\


The conditions needed to perform the two-sample $t$-test for independent groups are:
\begin{itemize}
  \item The two samples are independent
  \item Either:
        \begin{itemize}
          \item Both populations are normal and both samples are random
          \item Either population is not normal but the sample sizes are large enough
        \end{itemize}
\end{itemize}

\subsubsection{Step 3 - Finding the $p$-value}

The $p$-value is the probability of observing a difference in sample means as extreme as the one observed, assuming $H_0$
is true.

\subsubsection{Step 4 - Drawing Conclusions}

In drawing conclusions about the null hypothesis, we compare the $p$-value to the level of significance $\alpha $, and
state our conclusion as follows:
\begin{itemize}
  \item $p\text{-value} > \alpha$ - We do not have enough evidence against $H_0$ and cannot reject it.
  \item $p\text{-value} < \alpha$ - We have enough evidence against $H_0$ and can reject it in favour of $H_a$.
\end{itemize}

\subsection{Two sample $t$-test - Matched Pairs}

\subsubsection{Step 1 - Stating the Hypothesis}

For matched pairs, the hypothesis represents the difference in the population means $\left( \mu_d \right) $ of the two
quantitative variables, i.e.:
\begin{itemize}
  \item Null Hypothesis - $H_0: = \mu_{d}$
  \item Alternative Hypothesis:
        \begin{itemize}
          \item $H_a: \mu_d \neq 0$ (two-sided)
          \item $H_a: \mu_d > 0$ (one-sided)
          \item $H_a: \mu_d < 0$ (one-sided)
        \end{itemize}
\end{itemize}

\subsubsection{Step 2 - Collecting and Summarizing Data}

In this step we calculate the test statistic, which is in this case the $t$-score of the difference in the sample means,
i.e.:
\[
  t = \frac{\overline{x_d} - 0}{\frac{s_d}{\sqrt{n}}}
\]

Where $\overline{x_d}$ is the sample mean of the differences, and $s_d$ is the sample standard deviation of the differences.

\subsubsection{Step 3 - Finding the $p$-value}

The $p$-value is the probability of observing a difference in sample means as extreme as the one observed, assuming
$H_0$ is true, this is calculated using the $t$-score of the sample mean with the $t$-distribution with $n-1$ degrees of
freedom.

\subsubsection{Step 4 - Drawing Conclusions}

In drawing conclusions about the null hypothesis, we compare the $p$-value to the level of significance $\alpha $, and
state our conclusion as follows:
\begin{itemize}
  \item $p\text{-value} > \alpha$ - We do not have enough evidence against $H_0$ and cannot reject it.
  \item $p\text{-value} < \alpha$ - We have enough evidence against $H_0$ and can reject it in favour of $H_a$.
\end{itemize}

\subsubsection{Confidence Interval for $\mu_d$}

The point estimator used in this confidence interval is the sample mean of the differences $\overline{x_d}$

If the null value $0$ falls outside the confidence interval, then we can reject $H_0$

If the null value $0$ falls inside the confidence interval, then we cannot reject $H_0$

\subsection{ANOVA (Analysis of Variance) Test}

The ANOVA test is used to compare the means of three or more populations, and is used to determine if there is a
difference in the means of the populations. The test statistic used in the ANOVA test is the $F$-score, which is the ratio
of the variance between the sample means to the variance within the samples, i.e.:

\subsubsection{Step 1 - Stating the Hypothesis}

The null and alternative hypothesis for the ANOVA test are:
\begin{itemize}
  \item Null Hypothesis: $H_0: \mu_1 = \mu_2 = \mu_3 = \ldots = \mu_k$ / There is no relationship between the
        categorical variable and the quantitative variable
  \item Alternative Hypothesis: $H_a: \text{ not all the $\mu$'s are equal}$
\end{itemize}

\subsubsection{Step 2 - Checking conditions and finding the test statistic}

For the ANOVA test the test statistic is the $F$-score, which is the ratio of the variance between the sample means
to the variance within the samples, i.e.:
\[
  F = \frac{\text{Variation among sample means}}{\text{Variation within samples}}
\]
The larger the $F$-score, the stronger the evidence against the null hypothesis. If the variation within samples is
large then the $F$-score will be small, and vice versa.

The conditions needed to perform the ANOVA test are:
\begin{itemize}
  \item Random samples
  \item The sample size is large enough or the variable varies normally in the population
\end{itemize}

\subsubsection{Step 3 - Find the $p$-value}

The $p$-value is the probability of observing a difference in sample means as extreme as the one observed, assuming
$H_0$ is true.

\subsubsection{Step 4 - Drawing Conclusions}

In drawing conclusions about the null hypothesis, we compare the $p$-value to the level of significance $\alpha $,
and state our conclusion as follows:
\begin{itemize}
  \item $p\text{-value} > \alpha$ - We do not have enough evidence against $H_0$ and cannot reject it.
  \item $p\text{-value} < \alpha$ - We have enough evidence against $H_0$ and can reject it in favour of $H_a$.
\end{itemize}

\section{Case $C \to C$}

\subsection{Step 1 - Stating the Hypothesis}

In stating the null and alternative hypothesis for a relationship between two categorical variables, the null hypothesis
and alternative hypothesis take the following forms:
\begin{itemize}
  \item $H_0:$ There is no relationship between the two categorical variables / They are independent / $p_1 = p_2$
  \item $H_a:$  There is a relationship between the two categorical variables / They are dependent / $p_1 \neq p_2$
\end{itemize}

\subsection{Step 2 - Checking Conditions and Finding the Test Statistic}

The test statistic of the chi-square test for independence is the $\chi^2$-score, which is is a standardized measure of
the difference between the observed and expected frequencies, i.e.:
\[
  \chi^2 = \displaystyle\sum_{\text{all cells}} \frac{\left( \text{Observed Count} - \text{Expected Count} \right)^2 }{\text{Expected Count}}
\]

The conditions for the chi-square test for independence are:
\begin{itemize}
  \item Random sample
  \item The expected count in each cell is at least $5$
\end{itemize}

\subsection{Step 3 - Finding the $p$-value}

The $p$-value is the probability of observing a difference in sample means as extreme as the one observed, assuming
$H_0$ were true.

\subsection{Step 4 - Drawing Conclusions}

In drawing conclusions about the null hypothesis, we compare the $p$-value to the level of significance $\alpha $,
and state our conclusion as follows:
\begin{itemize}
  \item $p\text{-value} > \alpha$ - We do not have enough evidence against $H_0$ and cannot reject it.
  \item $p\text{-value} < \alpha$ - We have enough evidence against $H_0$ and can reject it in favour of $H_a$.
\end{itemize}

\section{Case $Q \to Q$}

\subsection{Step 1 - Stating the Hypothesis}

For a relationship between two quantitative variables, the null and alternative hypothesis take the form of, there is no
linear relationship between the two variables, and there is a linear relationship between the two variables, respectively i.e.:
\begin{itemize}
  \item Null Hypothesis : No linear relationship exists between $X$ and $Y$
  \item Alternative Hypothesis: A linear relationship exists between $X$ and $Y$
\end{itemize}
Where $\rho $ is the population correlation coefficient

\subsection{Step 2 - Collecting and Summarizing Data}

In this step we calculate the sample correlation coefficient $r$, and relevant sample statistics and summarize the data
with a test statistic. This test statistic is the $t$-score of the sample correlation coefficient, assuming $H_0$ is
true.

\subsection{Step 3 - Finding the $p$-value}

In finding the $p$-value for the population correlation coefficient, we use the $t$-score of the sample correlation
coefficient.


\subsubsection{Conditions for the $t$-test for the Population Correlation Coefficient}

The conditions needed to perform the $t$-test for the population correlation coefficient are:
\begin{itemize}
  \item The observed data seems to have a linear relationship
  \item The observations are independent
  \item There are no extreme outliers in the data
  \item The sample size is fairly large
\end{itemize}

\subsection{Step 4 - Drawing Conclusions}

In drawing conclusions about the null hypothesis, we compare the $p$-value to the level of significance $\alpha $,
and state our conclusion as follows:
\begin{itemize}
  \item $p\text{-value} > \alpha$ - We do not have enough evidence against $H_0$ and cannot reject it.
  \item $p\text{-value} < \alpha$ - We have enough evidence against $H_0$ and can reject it in favour of $H_a$.
\end{itemize}



\end{document}
