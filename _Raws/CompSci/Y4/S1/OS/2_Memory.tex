\documentclass[12pt letter]{report}
\input{./template/preamble}
\input{./template/macros}
\input{./template/letterfonts}

\title{\Huge{Memory Manager}}
\author{\huge{Madiba Hudson-Quansah}}
\date{}
\usepackage{parskip}
\usepackage{tabularx}

\setcounter{tocdepth}{4}
\setcounter{secnumdepth}{4}

\begin{document}
\maketitle
\newpage
\pdfbookmark[section]{\contentsname}{too}
\tableofcontents
\pagebreak

\chapter{Early Memory Management Systems}

\section {Single-User Contiguous Scheme}

\begin{itemize}
  \item Entire program is loaded into memory
  \item  Entirely contiguous allocation of memory space
  \item Jobs are processed sequentially
  \item The memory manager performs minimal work
    \begin{itemize}
      \item Evaluates incoming process size, loading jobs if small
        enough to fit in memory
      \item Monitors occupied memory space and clears entire memory
        space when a job is completed
    \end{itemize}
\end{itemize}

In this scheme before a job can be executed it must be loaded in its
entirely into memory and is allocated as much contiguous memory space
in  memory as it requires.  If the program is too lage to fit into
the available memory space it cannot be begin execution.

Single-user systems in a non-networked environment allocated to each
user, access to all available main  memory for each job. To allocate
memory the memory manager performs the following steps:
\begin{enumerate}
  \item Evaluate the incoming job to see if it small enough to fit
    into the available memory space. If it is load it into memory,
    else reject it and evaluate the next incoming process. A rejected
    job is never reconsidered as it can never fit into the available
    memory space.
  \item Monitors the occupied memory space. When the resident job
    ends its execution and no longer needs to be in memory, indicate
    that the entire amount of main memory space is now available and
    return to step 1.
\end{enumerate}

\subsection{Advantages}
\begin{itemize}
  \item Simple to implement
\end{itemize}

\subsection{Disadvantages}
\begin{itemize}
  \item Multiprogramming and networking is not possible, as only one
    job can be in memory at a time
  \item Not cost effective, as memory is often idle
\end{itemize}

\section{Fixed / Static Partition Scheme}

\begin{itemize}
  \item Memory is divided into fixed number of partitions, where each
    partition handles one job and reconfiguration requires system shutdown
  \item This partitioning scheme requires protecting each job's
    memory space, and matching jobs sizes with partition sizes
  \item The memory manager allocates memory space to jobs with job
    information stored in a table
    \begin{itemize}
      \item Multiprogramming is possible
      \item  Uses the first available partition with required size
        method for allocating memory
      \item Requires contiguous loading of entire program
      \item To work well all jobs should have similar size and memory
        size known in advance
      \item Arbitrary partition sizes can lead to internal
        fragmentation, i.e. wasted space within a partition
    \end{itemize}
\end{itemize}

\dfn{Internal Fragmentation}{
  Unused space inside a partition. Less than complete use of memory
  space within a partition
}

The first attempt to create a scheme that allows multiprogramming.
The memory is divided into a fixed number of partitions, where the
entirety of each partition is assigned to a single job, this allows
multiple jobs to execute at the same time. To allocate memory the
memory manager performs the following steps assuming there are two
partitions but this generalizes to any number of partitions:
\begin{enumerate}
  \item Check the incoming job's memory requirements. If it's greater
    then the size of the largest partition, reject the job and go to
    the next waiting job else go to step 2.
  \item  Check the job size against the size of the first available
    partition. If the job is small enough to fit, see if the
    partition is free. If it is, load the job into that partition
    else go to step 3.
  \item Check the job size against the size of the second available
    partition. If the job is small enough to fit, check to see if
    that partition is free. If it is available, load the incoming job
    into that partition else go to step 4.
  \item No partition is available now, so place the incoming job into
    the waiting queue for loading at a later time.
\end{enumerate}

In order to allocate memory spaces to jobs the memory manager must
have a table showing each partition's size, address, access
restrictions, and its current status (free or busy). For example:
\begin{table}[h!]
  \begin{center}
    \begin{tabular}{|c c c c c|}
      \hline
      Partition Size & Memory Size & Access & Partition Status \\ [0.5ex]
      \hline
      \hline
      100K           & 200K        & Job 1  & Busy             \\
      50K            & 300K        &        & Free             \\
      \hline
    \end{tabular}
  \end{center}
\end{table}

\subsection{Advantages}
\begin{itemize}
  \item More flexible than the single-user scheme as it allows
    multiple jobs to execute concurrently
\end{itemize}

\subsection{Disadvantages}
\begin{itemize}
  \item Requires the entire program to be loaded contiguously into memory
\end{itemize}

\section{Dynamic Partition Scheme}

\begin{itemize}
  \item Memory is partitioned dynamically as jobs arrive, i.e. a
    partition conforms to the size of the job
  \item Jobs are allocated on a first come, first served basis
  \item After the first partition sizing and allocation, all
    subsequent jobs are allocated using those partitions that are
    free and large enough to hold the job
\end{itemize}

\dfn{External Fragmentation}{
  Unused space between partitions. Less than complete use of memory
  space between partitions
}

Memory is rationed to an incoming jobs in one contiguous block, with
each job given the exact amount of memory it requires. Works well
when the first jobs are loaded and partitions form based on the sizes
of those first jobs, but when the next jobs arrive, which are not the
same size as those just deallocated, they are allocated space in the
available partition spaces on a priority basis. Jobs allocated in
memory are said to be in a partition exactly the size of the job.
This means that there can only be unused space between partitions,
i.e. internal fragmentation is not possible here.

\subsection{Advantages}
\begin{itemize}
  \item Reduces internal fragmentation, as partitions are sized to
    fit jobs exactly
  \item More flexible than fixed-partition scheme, as partitions are
    created dynamically
\end{itemize}

\subsection{Disadvantages}
\begin{itemize}
  \item Full memory utilization only occurs when the first jobs are
    loaded or if a job exactly fits a free partition
  \item External fragmentation can occur between partitions
\end{itemize}

\section{First-Fit Allocation}
\dfn{First-Fit Allocation}{
  Free and Busy lists are organized by memory locations, from low to
  high order memory addresses
}

\begin{itemize}
  \item Jobs are assigned to the first available partition large
    enough to hold it
  \item Fast, as it searches from the beginning of memory and stops
    when a large enough partition is found
\end{itemize}

For both fixed and dynamic partition schemes, the operating system
must keep track of each memory location's status, i.e. free or busy.
This is done using lists that track memory partitions and their
corresponding memory locations. These are the called the Free and
Busy lists. For the first-fit allocation method both lists are
organized by memory locations, from low to high order memory
addresses, i.e. from 0 (if not reserved) to the highest memory address.

When a job comes in the memory manager  compares jobs sizes to the
free list, allocating the first partition that is large enough to
hold the job. If the entire list is searched and finds no memory
block large enough to hold the job, the job is placed into a waiting
queue, and the memory manager fetches the next job in the queue.

\subsection{Advantages}
\begin{itemize}
  \item Faster allocation
\end{itemize}

\subsection{Disadvantages}
\begin{itemize}
  \item Can lead to many small unusable partitions at the beginning of memory
\end{itemize}

\section{Best-Fit}

\dfn{Best-Fit}{
  Free and Busy lists are organized by partition size, from smallest to largest
}

\begin{itemize}
  \item Jobs are assigned to the smallest available partition large
    enough to hold it
  \item More efficient use of memory, as it searches the entire list
    to find the smallest
\end{itemize}

For the best-fit allocation method both lists are organized by
partition size, from smallest to largest. When a job comes in the
memory manager compares jobs sizes to the free list, allocating the
smallest partition that is large enough to hold the job. If the
entire list is searched and finds no memory block large enough to
hold the job, the job is placed into a waiting queue, and the memory
manager fetches the next job in the queue.

\subsection{Advantages}
\begin{itemize}
  \item Best use of memory space
\end{itemize}

\subsection{Disadvantages}
\begin{itemize}
  \item Slower allocation, as it searches the entire free list
\end{itemize}

\section{Deallocation}

\dfn{Block}{
  A contiguous region of memory (a contiguous range of addresses)
  treated as a unit by the allocator; it can be either a free hole or
  an allocated partition.
}

\dfn{Deallocation}{
  Releasing allocated memory space
}

\subsection{Fixed and Dynamic Partition Deallocation}
For a fixed-partition system deallocation is trivial as partition
sizes are fixed so the partition's busy flag is set to free.

For a dynamic-partition system, the goal of deallocation is reduce
external fragmentation, there are three dynamic partition system
cases depending on the location of the to-be-freed block:
\begin{enumerate}
  \item Adjacent to another free block
  \item Between two free blocks
  \item Isolated from other free blocks
\end{enumerate}

\subsection{Joining Two Adjacent Free Blocks}

The block to be freed is adjacent to another free block. In this case
the two blocks are joined to form a larger free block, with the new
block's beginning address being the smallest beginning address. For
example this free list:
\begin{table}[H]
  \begin{center}
    \begin{tabular}{|c c c|}
      \hline
      Beginning Address & Memory Block Size & Status \\ [0.5ex]
      \hline
      \hline
      7560              & 20                & Free   \\
      (7600)            & (500)             & (Busy) \\
      *8100             & 100               & Free   \\
      9000              & 200               & Free   \\
      \hline
    \end{tabular}
  \end{center}
\end{table}
Where the block at address 7600 is to be freed. The resulting free list is:
\begin{table}[H]
  \begin{center}
    \begin{tabular}{|c c c|}
      \hline
      Beginning Address & Memory Block Size & Status \\ [0.5ex]
      \hline
      \hline
      7560              & 20                & Free   \\
      7600              & 510               & Free   \\
      9000              & 200               & Free   \\
      \hline
    \end{tabular}
  \end{center}
\end{table}

\subsection{Joining Three Adjacent Free Blocks}
The block to be freed is between two other free blocks. In this case
the three blocks are joined to form a larger free block, with the new
block's beginning address being the smallest beginning address. For
example this free list:
\begin{table}[H]
  \begin{center}
    \begin{tabular}{|c c c|}
      \hline
      Beginning Address & Memory Block Size & Status \\ [0.5ex]
      \hline
      \hline
      7560              & 20                & Free   \\
      *7600             & 500               & Free   \\
      (8100)            & (100)             & (Busy) \\
      *8200             & 200               & Free   \\
      10000             & 50                & Free   \\
      \hline
    \end{tabular}
  \end{center}
\end{table}
Where the block at address 8100 is to be freed. The resulting free list is:
\begin{table}[H]
  \begin{center}
    \begin{tabular}{|c c c|}
      \hline
      Beginning Address & Memory Block Size & Status       \\ [0.5ex]
      \hline
      \hline
      7560              & 20                & Free         \\
      7600              & 800               & Free         \\
      *                 &                   & (null entry) \\
      10000             & 50                & Free         \\
      \hline
    \end{tabular}
  \end{center}
\end{table}
We add a null entry to prevent the shifting all the entries in the
free list at the expense of memory.

\subsection{Isolated Free Block}
The block to be freed is isolated from other free blocks, i.e. it is
not adjacent to any other free block. In this case the block is added
to the free list at the appropriate location i.e. below the block
with the next lowest beginning address. For example this free list
and busy list:
% Free List
\begin{table}[H]
  \caption{Free List}
  \begin{center}
    \begin{tabular}{|c c c|}
      \hline
      Beginning Address & Memory Block Size & Status       \\ [0.5ex]
      \hline
      \hline
      1000              & 100               & Free         \\
      *                 &                   & (null entry) \\
      2000              & 200               & Free         \\
      \hline
    \end{tabular}
  \end{center}
\end{table}

% Busy List (in address order)
\begin{table}[H]
  \caption{Busy List}
  \begin{center}
    \begin{tabular}{|c c c|}
      \hline
      Beginning Address & Memory Block Size & Status \\ [0.5ex]
      \hline
      \hline
      1100              & 300               & Busy   \\
      1400              & 250               & Busy   \\
      1650              & 300               & Busy   \\
      \hline
    \end{tabular}
  \end{center}
\end{table}

Where the block at address 8400 is to be freed. The resulting free
and busy lists are:
% Free List (after freeing 1400..1649)
\begin{table}[H]
  \caption{Free List}
  \begin{center}
    \begin{tabular}{|c c c|}
      \hline
      Beginning Address & Memory Block Size & Status \\ [0.5ex]
      \hline
      \hline
      1000              & 100               & Free   \\
      1400              & 250               & Free   \\ % newly freed (isolated)
      2000              & 200               & Free   \\
      \hline
    \end{tabular}
  \end{center}
\end{table}

% Busy List (after)
\begin{table}[H]
  \caption{Busy List}
  \begin{center}
    \begin{tabular}{|c c c|}
      \hline
      Beginning Address & Memory Block Size & Status       \\ [0.5ex]
      \hline
      \hline
      1100              & 300               & Busy         \\
      *                 &                   & (null entry) \\
      1650              & 300               & Busy         \\
      \hline
    \end{tabular}
  \end{center}
\end{table}

\chapter{Memory Management Includes Virtual Memory}

\section{Paged Memory Allocation}

\begin{itemize}
  \item Incoming jobs are divided into pages of equal size
  \item Pages are loaded into page frames in main memory
  \item In the best case pages, sectors and page frames are the same
    size, with sizes determined by a disk's sector size
  \item The memory manager prior to program execution:
    \begin{itemize}
      \item Determines the number of pages in a program
      \item Locates enough empty page frames in main memory
      \item Loads all program pages into page frames
    \end{itemize}
  \item Programs can be stored in non-contiguous page frames
  \item Internal fragmentation can occur if a page is not completely
    filled and only happens on the job's last page
\end{itemize}

\dfn{Sector}{\label{def:sector}
  A fixed-length contiguous block of data on a disk
}

\dfn{Page}{\label{def:page}
  An equal sized division of a job.
}

\dfn{Page Frame / Frame}{
  A fixed sized division of main memory that holds a page.
}

Paged memory allocation, is based on the idea that jobs are divided
into units of equal size called pages. These pages are loaded into
memory occupying page frames. The size of a page frame is determined
by the size of a disk's sectors, as pages are often read from disk
into memory. Pages can be stored non-contiguously in main memory. The
memory manager prior to program execution performs the following steps:
\begin{itemize}
  \item Determines the number of pages in a job
  \item Locates enough empty page frames in main memory
  \item Loading all of the job's pages into page frames
\end{itemize}

\ex{}{
  A job of size 350 bytes is to be loaded into memory using paged
  memory allocation, where the page size is 100 bytes. The job is
  divided into 4 pages, each 100 bytes except the last page which is
  50 bytes. There is internal fragmentation of 50 bytes in the last
  page of the job when loaded into memory.
}

There are three tables used to track pages:
\begin{itemize}
  \item Job Table (JT) - Stores information for each active job
    \begin{itemize}
      \item Job Size
      \item Memory location of the job's PMT
    \end{itemize}
  \item Page Map Table (PMT) - Stores information for each page in a
    job, every active job has a PMT
    \begin{itemize}
      \item Page number starting from 0
      \item Memory address of the page frame where the page is loaded
    \end{itemize}
  \item  Memory Map Table (MMT) - Stores information for each page
    frame in main memory
    \begin{itemize}
      \item The locations of the page of which this frame is holding
      \item  Free/Busy status of each frame
    \end{itemize}
\end{itemize}

\subsection{Page Displacement}

\dfn{Line / Byte / Word}{
  The smallest unit of data that can be transferred between main
  memory and the CPU. A page frame is made up of multiple lines. Also
  called a word or byte depending on the architecture.
}

\dfn{Page Displacement / Offset}{
  The distance of a line from the beginning of a page. It is a
  relative factor used to locate a certain line within its page frame.
}

To determine the page number and displacement of a line we:
\begin{enumerate}
  \item Divide the job space address by the page size
  \item The page number is the integer quotient
  \item The displacement is the remainder
\end{enumerate}
I.e.:
\begin{align*}
  \text{Page Number}  & = \left\lfloor \frac{\text{Job Space
  Address}}{\text{Page Size}} \right\rfloor \\
  \text{Displacement} & = \text{Job Space Address} \mod \text{Page
  Size}                               \\
\end{align*}

\ex{}{
  \qs{}{
    With a page size of 4096 bytes find the page and displacement of line 7149
  }

  \sol{
    \begin{align*}
      \text{Page Number}  & = 7149 \div 4096  \\
      & = 1               \\
      \text{Displacement} & = 7149 \bmod 4096 \\
      & = 3053            \\
    \end{align*}
  }
}

To determine the exact location of an instruction or data item in
main memory we:
\begin{enumerate}
  \item Determine the page number/displacement of the line
  \item Refer to the job's PMT to determine the page frame containing
    the required page
  \item Obtain the beginning address of the page frame
  \item Multiply the page frame number by the page size
  \item Add the displacement to the starting address of the page frame
\end{enumerate}
This is also called address resolution / translation converting a
logical address (job space address) to a physical address (main memory address).

\subsection{Advantages}
\begin{itemize}
  \item Pages don't have to be loaded contiguously
  \item Efficient use of memory, as jobs are loaded into any
    available page frame
  \item Compaction and relocation are not required
\end{itemize}

\subsection{Disadvantages}
\begin{itemize}
  \item Internal fragmentation can occur on the last page of a job
  \item Additional overhead is required for address translation
  \item Requires entire job to be loaded into memory before execution can begin
\end{itemize}

\section{Demand Paging Memory Allocation}

\begin{itemize}
  \item Loads only a part of the program into memory
  \item Exploits programming techniques where only a small part of
    the program is needed at any one time
  \item Simulates a larger amount of memory than is physically
    available, i.e. Virtual Memory
  \item Modifies PMT to include:
    \begin{itemize}
      \item If the page is already in memory
      \item Are the page contents modified
      \item Has the page been referenced recently
      \item Page Frame Number
    \end{itemize}
  \item Swapping / Paging is used to move pages between main memory
    and secondary memory
    \begin{itemize}
      \item When a page is needed that is not in memory a page fault occurs
      \item A resident memory page is freed based on a policy
      \item If the resident page has been modified it is copied to
        secondary memory
      \item The new page is copied into the freed page frame
    \end{itemize}
\end{itemize}

\dfn{Page Fault}{
  The event that occurs when a program tries to access a page that is
  not currently in main memory, causing a page interrupt
}

\dfn{Page Interrupt}{
  An interrupt generated when a page fault occurs, causing the
  operating system to fetch the required page from secondary memory
  into main memory
}

\dfn{Swapping / Paging}{
  The process of moving pages between main memory and secondary memory
}

\dfn{Thrashing}{
  Excessive swapping of pages between main memory and secondary
  memory, leading to a significant decrease in system performance.
  Mainly occurs when:
  \begin{itemize}
    \item There is insufficient memory to hold the working set of a
      process, i.e. large number of jobs and limited free pages
    \item The operations of pages cross page boundaries frequently,
      i.e. a loop that spans multiple pages
  \end{itemize}
}

The demand paging scheme loads only parts of a job into memory at any
given time. With demand paging jobs are still divided into equally
sized pages but pages are only loaded into memory when needed. Demand
paging takes advantage of the usually sequential nature of program execution.

Successful demand paging implementation requires the use of
high-speed Direct Access Storage Devices (DASD), as pages must be
quickly passed from secondary storage into main memory as needed.
Demand paging modifies the PMT to include additional information
about each page:
\begin{itemize}
  \item If the page is already in memory - Contains where the page is
    located in memory if it has been loaded into memory.
  \item Have the page contents been modified -  Indicates if the page
    has been changed since it was loaded into memory, it is used to
    save time when pages are removed from main memory and saved back
    to secondary memory. If the page hasn't been modified it doesn't
    have to saved back to secondary storage saving time.
  \item Has the page been referenced recently - Used to determine
    which pages show the most processing activity and which are
    relatively inactive. This information can be used by page
    replacement policies to determine which pages should remain in
    main memory and which should be swapped.
  \item The page frame number
\end{itemize}

\dfn{Page Fault Handler}{
  Determines whether there are empty page frames in memory so that
  the requested page can be swapped into memory. If all page frames
  are busy then the page fault handler must decide which page with be
  swapped out of memory based on a page replacement policy.
}

When a program tries to access a page that is not currently in memory
a page fault occurs, causing a page interrupt. The section of the
operating system that resolves page faults its called the page fault handler.

Excessive swapping can reduce performance as the system spends more
time swapping pages in and out of memory than executing jobs. This is
called thrashing. Thrashing can occur in several instances:
\begin{itemize}
  \item When a large umber of jobs are supposed to execute with a
    relatively small number of free page frames
  \item When a job's operations cross page boundaries frequently,
    e.g. a loop that spans multiple pages
\end{itemize}

\subsection{Page Replacement Policies}
\begin{itemize}
  \item First-In-First-Out (FIFO)
    \begin{itemize}
      \item The oldest page in memory is replaced
    \end{itemize}
  \item Least Recently Used (LRU)
    \begin{itemize}
      \item The page that has not been used for the longest time is replaced
    \end{itemize}
\end{itemize}

A page replacement policy is used to determine which page in memory
should be swapped out to resolve a page fault.

\subsubsection{First-In First-Out (FIFO)}
\begin{itemize}
  \item Removes the page that has been in memory the longest
  \item Failure rate is determined by the ratio of page interrupts to
    page requests
  \item More memory does not guarantee better performance (Belady's Anomaly)
\end{itemize}

The first-in first-out policy will remove the pages that have been in
memory the longest, i.e. those that were first in. Each time a needed
page is not found in memory it wall cause an interrupt, leading to
swapping. We can count the number of interrupts and use it to
determine the success and failure rates of a page replacement policy, i.e.:
\begin{align*}
  \text{Success Rate} & =  \frac{\text{Page Requests Made} -
  \text{Number of interrupts}}{\text{Page Requests Made}} \\
  \text{Failure Rate} & =  \frac{\text{Number of
  interrupts}}{\text{Page Requests Made}}
\end{align*}

FIFO suffers from Belady's Anomaly, where increasing the number of
page frames, i.e. buying more memory, does not guarantee better performance.

\subsubsection{Least Recently Used (LRU)}
\begin{itemize}
  \item Removes the page that has not been used for the longest time
  \item Takes advantage of the principle of locality, i.e. if a page
    has not been used for a long time it is unlikely to be used in
    the near future
  \item More memory guarantees better performance
  \item Has various implementations
    \begin{itemize}
      \item Clock Replacement - A pointer steps through active pages'
        reference bits and replaces the first page with a reference bit of 0
      \item Bit-Shifting - Each page has an 8-bit register, every
        time a page is referenced its register is shifted right by
        one bit and a 1 is placed in the leftmost bit. The page with
        the smallest value is replaced
    \end{itemize}
\end{itemize}

\dfn{Principle of Locality}{
  The tendency of a program to access a relatively small portion of
  its address space at any given time. This means that if a program
  accesses a particular memory location, it is likely to access
  nearby memory locations in the near future.
}

\dfn{Temporal Locality}{
  The tendency of a program to access the same memory location
  multiple times within a short period of time. This means that if a
  program accesses a particular memory location, it is likely to
  access that same memory location again in the near future.
}

The lest recently used (LRU) policy swaps out the pages that show the
least recent activity, taking advantage of the principle of locality,
specifically temporal locality.

\nparagraph{Clock Replacement Variation}

\nparagraph{Bit-Shifting Variation}

\subsection{Working Set}

\begin{itemize}
  \item Loads a set of related pages into memory allowing direct
    access without incurring a page fault
  \item Takes advantage of the principle of locality, i.e. programs
    tend to use a small set of pages intensively for a period of time
  \item Requires the system define the number of pages that makes up
    a working set and the maximum number of pages allowed in a working set.
\end{itemize}

\subsection{Advantages}
\begin{itemize}
  \item Reduces the amount of memory required by a job
  \item Reduces the time required to load a job into memory
  \item Allows larger jobs to be run in memory
\end{itemize}

\subsection{Disadvantages}
\begin{itemize}
  \item Requires high-speed page access
\end{itemize}

\section{Segmented Memory Allocation}

\dfn{Segment}{
  A logical unit of a program containing code the performs related
  functions, e.g. main program, subroutine, data table, etc.
}

\begin{itemize}
  \item A program is divided into segments of variable length
  \item Each segment is loaded into a memory partition large enough to hold it
  \item Segments are loaded non-contiguously
  \item A segment table (ST) is used to track segments
\end{itemize}

Segmented memory allocation divides a program into logical units
called segments, which each segment containing logical groupings of
code. Segments can be of differing sizes, and are loaded into
dynamically allocated memory partitions. Segmented Memory Allocation
differs from Paged Memory Allocation in that segments can be of
varying sizes, while pages are of fixed size and main memory is not
divided into page frames and segments are allocated like dynamic
partitions. When a job arrives the memory manager divides the program
into segments, and stored in a Segement Map Table (SMT) which contains:
\begin{itemize}
  \item Segment Number
  \item Segment Size
  \item Access Rights
  \item Status
  \item Memory Address
\end{itemize}
The memory manager also maintains a Job Table, and a Memory Map
Table. To access a specific location within a segment we perform a
similar process to paged memory allocation:
\begin{align*}
  \text{Segment Number} & = \left\lfloor \frac{\text{Job Space
  Address}}{\text{Segment Size}} \right\rfloor \\
  \text{Displacement}   & = \text{Job Space Address} \mod
  \text{Segment Size}                               \\
\end{align*}
As segments can vary in size the displacement bys be verified to make
sure it isn't outside the segment's range.

\subsection{Advantages}
\begin{itemize}
  \item Reduces internal fragmentation, as segments are sized to fit
    jobs exactly
  \item More flexible than fixed-partition scheme, as partitions are
    created dynamically
  \item Segments can be shared between jobs
  \item Segments can be protected with access rights
\end{itemize}

\subsection{Disadvantages}
\begin{itemize}
  \item External fragmentation can occur between segments
  \item Requires entire job to be loaded into memory before execution can begin
  \item More complex to implement than paged memory allocation
\end{itemize}

\section{Segmented/Demand Paged Memory Allocation}

Each segment is divided into pages, and only the required pages of a
segment are loaded into memory. This requires four types of tables:
\begin{description}
  \item[Job Table (JT)] - Lists every job in process, i.e. one JT for
    the whole system
  \item[Segment Map Table (SMT)] - Lists details about each segment,
    i.e. one SMT per job
  \item[Page Map Table (PMT)] - Lists details about every page, i.e.
    one PMT per segment
  \item[Memory Map Table (MMT)] - Monitors the allocation of the page
    frames in main memory, i.e. one MMT for the whole system
\end{description}

To access a certain location in memory now, the system locates the
address, which is composed of three entries:
\begin{itemize}
  \item Segment Number
  \item Page Number within the segment
  \item Displacement within the page
\end{itemize}

\dfn{Associative Memory }{
  Registers that are allocated to each job that is active, whose task
  to associate several segment and page numbers, belonging to the job
  being processed, with their main memory address.
}

Associative registers exist as hardware within the CPU to speed up
the address translation process. I.e.:
\begin{enumerate}
  \item When a page is first requested the job's SMT is searched to
    locate its PMT.
  \item The PMT is loaded, if not already present, and searched to
    determine the page's location in memory
    \begin{enumerate}
      \item If the page isn't in memory, then a page interrupt occurs
        and it's brought into memory, and the table is updated.
      \item Since this segment's PMT now resides in memory, any other
        requests for page within this segment can be quickly
        resolved because there is no need to access the SMT again to
        load the PMT. However accessing these table is time consuming
    \end{enumerate}
\end{enumerate}

With associative memory, when a page is first requested two searches
can run in parallel:
\begin{itemize}
  \item A search of the SMT to locate the PMT
  \item A search of the associative memory to locate the page frame
    number based on the segment number, page number, and job ID
\end{itemize}

If the associative memory search finishes first the SMT search is
aborted and the address translation is performed using the page frame
number obtained from the associative memory. If the SMT search
finishes first, the PMT is loaded into memory and the address
translation is performed using the page frame number obtained from
the PMT. In either case, if the page is not in memory a page
interrupt occurs and the required page is loaded into memory.

\subsection{Advantages}

\begin{itemize}
  \item Logical benefits of segmentation, i.e. sharing and protection
  \item Physical benefits of demand paging, i.e. reduced memory
    requirements,
  \item Removes the problems of compaction, external fragmentation,
    and secondary storage handling due to the fixed page size
\end{itemize}

\subsection{Disadvantages}

\begin{itemize}
  \item Increased overhead required to manage multiple tables
  \item Increased overhead required for address translation
  \item More complex to implement than either segmentation or demand
    paging alone
\end{itemize}

\section{Virtual Memory}

Virtual memory gives the illusion of a very large main memory by using
secondary memory to extend main memory. This allows very large jobs
to be executed even though they require large amounts of main memory
that the hardware cannot support. All the allocation schemes
discussed in this chapter are forms of virtual memory, which mostly
falls into two categories, virtual memory with paging and virtual
memory with segmentation.

\begin{table}[H]
  \centering
  \begin{tabularx}{\textwidth}{|X|X|} % Force table to span text width
    \hline
    \textbf{Virtual Memory with Paging} & \textbf{Virtual Memory with
    Segmentation} \\ [0.5ex]
    \hline
    \hline
    Allows internal fragmentation within page frames & Doesn't allow
    internal fragmentation \\ \hline
    Doesn't allow external fragmentation & Allows external
    fragmentation between segments \\ \hline
    Programs are divided into equal-sized pages & Programs are
    divided into unequal-sized segments of logically grouped code \\ \hline
    The absolute address is calculated using the page number and
    displacement & The absolute address is calculated using the
    segment number and displacement \\ \hline
    Requires Page Map Table (PMT) & Requires Segment Map Table (SMT) \\
    \hline
  \end{tabularx}
  \caption{Comparison of Paging and Segmentation}
\end{table}

Segmentation allows users to share program code, with the shared
segment containing:
\begin{description}
  \item[Reentrant Code Area]  - An area where unchangeable code
    (reentrant code) is stored
  \item[Data Areas] - Several data areas, one for each user
\end{description}
Users share the code which cannot be modified but they can modify the
information stored in their own data areas as needed, without
affecting the data stored in other users' data areas.

\subsection{Advantages}
\begin{itemize}
  \item A job's size is not restricted to the size of main memory
  \item More efficient use of memory, as the only sections of a job
    stored in memory are those needed immediately
  \item It allows an unlimited amount of multiprogramming, which can
    apply to many jobs
  \item Allows the sharing of code and data
  \item Facilitates the dynamic linking of program segements
\end{itemize}

\subsection{Disadvantages}
\begin{itemize}
  \item Increased processor hardware costs
  \item Increased overhead of handling paging interrupts
  \item Increased software complexity to prevent thrashing
\end{itemize}

\section{Cache Memory}

\dfn{Cache}{
  A small, high-speed memory located close to the CPU that stores
  frequently accessed data and instructions to reduce the average time
  to access data from the main memory.
}

Because the cache is small in capacity compared to main memory, it
can use more expensive and faster memory chips, which match the speed
of the CPU. This allows frequently used instructions or data to be
accessed quickly improving overall system performance. A typical
microprocessor has two or more levels of cache:
\begin{description}
  \item[Level 1 (L1) Cache] - The smallest and fastest cache, located
    directly on the CPU chip. It is divided into separate instruction
    and data caches.
  \item[Level 2 (L2) Cache] - Larger than L1 but slower, it can be
    located on the CPU chip or on a separate chip close to the CPU.
  \item[Level 3 (L3) Cache] - Even larger and slower than L2, it is
    typically located on the motherboard and shared among multiple
    CPU cores.
\end{description}

When designing cache memory, one must take into consideration the
following factors:
\begin{description}
  \item[Cache Size]  - Larger caches can store more data but are more
    expensive and
    slower.
  \item[Block Size] - Because of the principle of locality, as block
    size increases the ratio of the number references found in the
    cache to the number of references tends to increase, meaning that
    larger block sizes can improve cache performance. However, larger
    block sizes also increase the likelihood of cache misses due to
    increased contention for cache space.
  \item[Block Replacement Algorithm] - When all cache blocks are
    occupied and a new block has to be brought into the cache, the
    block selected for replacement should be one that is least likely
    to be used in the near future. LRU is a common algorithm used for
    this purpose.
  \item[Rewrite Policy] - When the contents of a block residing in
    cache are changed it must be written back to main memory before
    it is replaced by another block. A rewrite policy must be in
    place to determine when this writing will take place. Two common
    policies are:
    \begin{description}
      \item[Write-Through] - Modified blocks are written back to main
        memory every time a change occurs, which would increase the
        number of memory writes.
      \item[Write-Back] - Modified blocks are written back to main
        memory only when the block is replaced or the process is
        finished, which would minimize overhead but would leaver the
        block in main memory in an inconsistent state until it is
        written back, creating a problem in multiprocessor environments.
    \end{description}
\end{description}

The measure of cache efficiency/performance is called the cache hit
ratio, and when shown as a percentage shows the rate of memory
accesses found in the cache. It is calculated as:
\begin{align*}
  \text{Cache Hit Ratio} &= \frac{\text{Number of requests found in
  the cache}}{\text{Total number of requests}}   \\
\end{align*}

Another measure of efficiency is the average memory access time (AMAT),
\begin{align*}
  AMAT &= \text{Average Cache Access Time} + \left( 1 - \text{Cache
  Hit Ratio} \right) \times \text{Average Main Memory Access Time}  \\
\end{align*}

\end{document}
