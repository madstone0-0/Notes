\documentclass[12pt letter]{report}
\input{./template/preamble}
\input{./template/macros}
\input{./template/letterfonts}

\title{\Huge{Decrease-and-Conquer}}
\author{\huge{Madiba Hudson-Quansah}}
\date{}
\usepackage{parskip}

\setcounter{tocdepth}{4}
\setcounter{secnumdepth}{4}

\begin{document}
\maketitle
\newpage
\pdfbookmark[section]{\contentsname}{too}
\tableofcontents
\pagebreak

\chapter{Introduction}

The decrease and conquer technique, exploits the relationship between a solution to a given instance of a problem and a
solution to its smaller instance. Once such a relationship is established, it can be exploited either top down or bottom
up. The top down approach naturally leads to a recursive implementation, also known as divide and conquer. The bottom up
variation is usually implemented iteratively, starting with the smallest solvable instance of a problem and building up
to the whole problem. This is also known as the \textbf{incremental approach}. There are three major variants of
decrease and conquer:
\begin{itemize}
  \item Decrease by a constant.
  \item Decrease by a constant factor.
  \item Variable size decrease.
\end{itemize}

\section{Decrease by a constant}

The size of the problem instance is reduced by the same constant on each iteration of the algorithm, usually by one.
Taking the example of the exponentiation problem, $a^{n}$ where $a \neq 0$ and $n \in \mathbb{N}$, the relationship
between a solution to a reduced problem instance, $n - 1$ and $n$ is obtained easily:
\[
  a^{n} = a \times a^{n-1}
\]
This allows the function $f \left( n \right) = a^{n} $ to be computed top down, recursively, by:
\[
  f \left( n \right) = \begin{cases}
    1                               & \text{if } n = 0 \\
    f \left( n -1  \right) \times a & \text{if } n > 0
  \end{cases}
\]
Or bottom up, iteratively, by:
\[
  f \left( n \right) = \prod_{i = 1}^{n} a
\]

\section{Decrease by a constant factor}

The size of a problem instance is reduced  by the same constant factor on each iteration of the algorithm, usually by a
factor of two. Using the exponentiation problem again we can observe that the relationship between a solution to the
reduced problem instance, $n/2$ and $n$ is:
\[
  a^{n} = \left( a ^{\frac{n}{2}} \right)^{2}
\]
But considering only integer values of $n$, this does not hold for odd values of $n$. If $n$ is odd, we have to compute
for $n-1$ instead, i.e.
\[
  a^{n} = \left( a^{\frac{n-1}{2}} \right)\times a
\]
This allows the function $f \left( n \right) = a^{n} $ to be computed top down, recursively, by:
\[
  a^{n} = \begin{cases}
    \left( a^{\frac{n}{2}} \right) ^2         & \text{if $n$ is even and positive} \\
    \left( a^{\frac{n-1}{2}} \right) \times a & \text{if $n$ is odd}               \\
    1                                         & \text{if $n = 0$}
  \end{cases}
\]
Or bottom up, iteratively, by:
\[
  a^{n} = \begin{cases}
    \prod_{i = 1}^{n/2} a \times a     & \text{if $n$ is even and positive} \\
    \prod_{i = 1}^{(n-1)/2} a \times a & \text{if $n$ is odd}               \\
    1                                  & \text{if $n = 0$}
  \end{cases}
\]
In both cases the number of operations done each step reduces by a factor of two, making the algorithm run in $\Theta
  \left( \log n \right) $ time.

\section{Variable size decrease}

The size of the problem instance is reduced by a variable amount on each iteration of the algorithm. An example of this
is Euclid's algorithm for finding the greatest common divisor of two numbers:
\[
  \gcd \left( m, n \right)  = \gcd \left( n, m\mod n \right)
\]
In this algorithm the reduction in the size of the problem instance depends on the size of the numbers $m$ and $n$

\chapter{Insertion Sort}

\dfn{Insertion Sort}{
  For an array of $n$ elements $A \left[ 0\ldots n-1 \right] $, we assume that the smaller problem instance of $A \left[
      0\ldots n-2\right] $ has already been solved, giving us a sorted array of size $n - 1$, $A_0 \leq \ldots \leq
    A_{n-2}$. Taking advantage of this, we just have $A_{n-1}$ left unsorted. We then scan the sorted subarray from the
  right to find the correct position for $A_{n-1}$ and insert it there. This is repeated for each increasing subarray
  until the entire array is sorted, i.e.:
  \[
    A_0 \leq \ldots \leq A_{n-i} \mid A_i,\, A_{n-1}
  \]
  Where $i$ is the number of passes through the array so far.
  Although presented as a top down approach, it is more efficiently implemented as a bottom up approach, i.e.
  iteratively, starting with $A_1$, and building up to $A_{n-1}$.
}

\begin{algorithm}[H]
  \caption{InsertionSort $ \left( A, n\right) $}
  \Comment{}\\
  \Comment{Sorts a given array by insertion sort} \\
  \Comment{Input: An array $A$ of $n$ orderable elements} \\
  \Comment{Output: Array $A$ sorted in nondecreasing order} \\
  \begin{algorithmic}[1]
    \For{ $i \gets 1$ to $n - 1$}
    \State $v \gets A_i$
    \State $j \gets i - 1$
    \While{ $j \geq 0$ and $A_j > v$}
    \State $A_{j+1} \gets A_j$
    \State $j \gets j - 1$
    \EndWhile
    \State $A_{j + 1} \gets v$
    \EndFor
  \end{algorithmic}
\end{algorithm}

The basic operation of this algorithm is the comparison $A_j > v$, and the number of comparisons done depends on the
nature of the input, i.e. if the array is already sorted the number of comparisons is $n-1$ as the while loop will never
iterate but the outer for loop will always run the whole length of the array. In the worst case, where the array is
sorted in descending order, the number of comparisons done is the largest amount possible, i.e. for every element
encountered the while loop will iterate $j$ times, where $j$ is the index of the element in the array. Calculating this:
\begin{align*}
  C \left( n \right) & = \displaystyle\sum_{i=1}^{n-1} \displaystyle\sum_{j=0}^{i-1} 6 \\
                     & = 6\displaystyle\sum_{i=1}^{n-1} \left( i - 1 \right) + 1       \\
                     & = 6 \displaystyle\sum_{i=1}^{n-1} i                             \\
                     & =  6 \left( \frac{n \left( n - 1 \right)}{2} \right) + 1        \\
                     & =  3n^{2} - 3n + 1
\end{align*}
Therefore the worst case time complexity of the insertion sort algorithm is $\Theta \left( n^{2} \right) $.

\chapter{Topological Sorting}

\dfn{Topological Sort}{
  Given a directed acyclic graph $G = \left( V, E \right) $, a topological sort is a linear ordering of the vertices
  such that for every directed edge $ \left( u, v \right) $ from vertex $u$ to vertex $v$, $u$ comes before $v$ in the
  ordering. This is useful in scheduling tasks that have dependencies on each other.
}

Topological sorting can be done in a decrease and conquer manner by the source removal algorithm. This algorithm works by
identifying in a remaining digraph a \textbf{source} vertex, i.e. a vertex with an in-degree of 0 and delete it with the
edges originating from it. This is repeated until all vertices have been removed. The order these vertices are removed
in is the topological sort of a digraph.

\chapter{Generating Combinatorial Objects}

\section{Generating Permutations}
In generating permutations we assume that the underlying set whose elements we want to permute is the set of integers
from $1$ to $n$, i.e., they can be represented as the indices of elements in an $n$-element set $\{a_1,\ldots,a_n\} $.\\
In breaking the task of generating all $n!$ permutation of the set $\{1, \ldots, n\} $, we can consider all $ \left( n -
  1\right)! $ permutations have been generated. Assuming this we can relate the task of generating all $n!$ permutations
to augmenting this set of $ \left( n - 1 \right)! $ permutations with the $n$th element in all possible positions, i.e.:
\[
  n! = \left( n - 1 \right)! \times n
\]
We can insert $n$ in the previously generated permutations either left to right or right to left. It is more efficient
to start with inserting $n$ into $1,2,\ldots \left( n-1 \right) $ by moving right to left, i.e. inserting $n$ at the end of
each permutation and switching direction every time a new permutation of $\{1, \ldots, n - 1\} $ needs to be
processed, i.e. for $n = 3$:

\begin{align*}
  \begin{array}{c c c c}
    \text{start}                  & 1   &     &     \\
    \text{insert 2 right to left} & 12  & 21  &     \\
    \text{insert 3 right to left} & 312 & 132 & 213 \\
    \text{insert 3 left to right} & 321 & 231 & 213
  \end{array}
\end{align*}

It is also possible to get the same ordering of permutations of $n$ without explicitly generating all permutations of
for smaller values of $n$. This is done by associating a direction with each element $k$ in a permutation. This element
$k$ is said to be \textbf{mobile}, if its direction points to a smaller element adjacent to it. This defines the
\textbf{Johnson-Trotter} algorithm for generating permutations:
\begin{algorithm}[H]
  \caption{JohnsonTrotter $ \left( n \right) $}
  \Comment{}\\
  \Comment{Implements Johnson-Trotter algorithm for generating permutations} \\
  \Comment{Input: A positive integer $n$} \\
  \Comment{Output: A list of all permutations of $\{1,\ldots,n\} $} \\
  \begin{algorithmic}[1]
    \State Initialize the first permutation with $1, 2, \ldots, n$ with all directions pointing left
    \While{the last permutation has a mobile element}
    \State find its largest mobile element $k$
    \State swap $k$ with the adjacent element it is pointing to
    \State reverse the direction of all the elements greater than $k$
    \State add the new permutation to the list of permutations
    \EndWhile
  \end{algorithmic}
\end{algorithm}

\section{Generating Subsets}

The task of generating all subsets of a set $A = \{a_1, \ldots, a_n\} $, can be done in a decrease and conquer manner by
dividing all the subsets of $A$ into two groups:: those that do not contain $a_n$ and those that do. The former group is
all the subsets of $\{a_1, \ldots, a_{n-1}\} $, while each and every element of the latter can be obtained by adding
$a_n$ to a subset of $\{a_1, \ldots, a_n\} $.

\chapter{Decrease-by-a-Constant-Factor}

\section{Binary Search}
\dfn{Binary  Search }{
  In a sorted array $A$ of $n$ elements we first compare the search key $K$ with the middle element of the array $A_m$.
  If they match, the algorithm stops, otherwise, the same operation is repeated recursively for the first half of the
  array if $K < A_m$, and for the second half if $K > A_m$:
  \[
    A_0 \ldots A_{m - 1}\, A_m \, A_{m + 1} \ldots A_{n - 1}
  \]
}

\begin{algorithm}[H]
  \caption{BinarySearch $ \left( A, n, K \right) $}
  \Comment{}\\
  \Comment{Implements nonrecursive binary search} \\
  \Comment{Input: An array $A$ of $n$ elements sorted in ascending order and a search key $K$} \\
  \Comment{Output: An index of the array's element that is equal to $K$ or $-1$ if there is no such element } \\
  \begin{algorithmic}[1]
    \State $l \gets 0$
    \State $h \gets n - 1$
    \While{ $l \le  h$}
    \State $m \gets \left\lfloor \left( h + l \right) \right\rfloor  / 2$
    \If{ $A_m = K$}
    \State \Return $m$
    \ElsIf{ $A_m > K$}
    \State $l \gets m + 1$
    \Else
    \State $r \gets m - 1$
    \EndIf
    \EndWhile
    \State \Return -1
  \end{algorithmic}
\end{algorithm}

The analysis of binary search  is focused on the number of times its basic operation, the comparison of the value and
the search key, is done.  This number varies depending on the nature of the input. In the worst case $C_{\text{worst}}
  \left( n \right) $, the input array does not contain the search key, or the search key is the last element of the array.
Since after one comparison the algorithm is left with an array of half size we get the following recurrence relation:
\[
  C_{\text{worst}} \left( n \right) = C_{\text{worst}} \left( \left\lfloor n / 2 \right\rfloor \right) + 1 \text{ for } n
  > 1, \, C_{\text{worst}} \left( 1 \right)  = 1
\]
We can solve this using the master theorem:
\[
  O \left( \log n \right)
\]

\section{Fake-Coin Problem}

\dfn{Fake-Coin Problem}{
  Among $n$ identical-looking coins, one is fake. With a balance scale, we can compare any two sets of coins, i.e. by
  tipping to the left to the right, or stating even, the balance scale will tell whether the sets weigh the same or
  which of the sets is heavier than the other but not by how much.
}

We can solve this problem in a Decrease-by-a-Constant-Factor manner by:
\begin{itemize}
  \item Dividing $n$ coinst into two piles of $\left\lfloor n / 2 \right\rfloor$ coins each , leaving one extra coin
        aside if $n$ is odd.
  \item Put the two pile on the scale. If the piles weigh the same, the coin put aside must be fake, otherwise, we can
        proceed in the same manner with the lighter pile which must contain the fake coin.
\end{itemize}
This gives us the following recurrence relation:
\[
  W \left( n \right)  = W \left( \left\lfloor n / 2 \right\rfloor \right)  + 1 \text{ for } n > 1, \, W \left( 1 \right)
  = 0
\]

This is identical to the binary search cost relation since both algorithms are based on the same
Decrease-by-a-Constant-Factor technique of decreasing by a factor of 2. This leads to the close form of this recurrence
also being:
\[
  W \left( n \right)  = \left\lfloor \log n \right\rfloor
\]

\section{Russian Peasant Multiplication}

\dfn{Russian Peasant Multiplication}{
  Let $n$ and $m$ be positive integers whose product we want to compute, and let us measure the instance size by the
  value of $n$. If $n$ is even, and instance of half the problem size has to deal with $n / 2$. This gives us this
  formula relating the smaller $n / 2$ multiplication to the larger problem instance
  \[
    n \cdot m = \frac{n}{2} \cdot 2 m
  \]
  If $n$ is odd, we need to account for that , i.e.:
  \[
    n \cdot m = \frac{n - 1}{2} \cdot 2 m + m
  \]
  Thus our base case becomes $n = 1$, i.e. $1 \cdot m = m$
}

\chapter{Variable-Size-Decrease}
\section{Computing a Median and the Selection Problem}

\dfn{Selection problem}{
  The problem of finding the $k$th smallest element in an array of $n$ elements. This number is also known as the  $k$th
  order statistic.
}

The median is a special case of the selection problem where $\left\lceil n / 2 \right\rceil $, which asks to find an
element divides the array into two equal halves. This can be done by sorting the array and selecting the middle element,
but this is not efficient as it takes $\Theta \left( n \log n \right) $ time.\\
We can instead use the idea of partitioning a given list around some value $p$, for example the first element. Then we
can identify all the elements less than $p$ and greater than $p$ to determine it's position in the sorted list. Using
the notion of \textbf{Lomuto partitioning}, we think of the subarray, $A \left[ l \ldots r \right] $, as being divided into three parts:
\[
  p \mid \text{less than $p$} \mid \text{greater than $p$} \mid \text{unprocessed}
\]
I.e. a segment with elements known to be less than $p$, a segment with elements known to be greater than $p$, and a
segment of elements yet to be compared to $p$.

\end{document}
